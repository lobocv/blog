[{"title":"Testing complex workflows in Go","date":"","description":"","body":"The standard library testing package in Go does many things well. It is simple to use and executes fast and efficiently, even running tests in parallel and the caching results. However, many including myself, have turned to supplemental packages to address some blind spots when writing tests:\nTest setup, teardown and grouping Assertions and output validation Mocking I have found that these blind spots become particularly cumbersome when you have workflows that involve sophisticated fixture setup, multiple edge cases, error handling, mocks, and complex outputs. In this article, I\u0026rsquo;m going to go through a style of testing that helped me with many of these issues.\nTestify One of the most popular third party testing packages is stretchr/testify, and for good reason. It boasts solutions to the major blind spots I\u0026rsquo;ve listed above by providing easy assertions, mocking and test suite interfaces and functions. In particular, I find myself using testify/suite for complex workflows because it provides better test setup, teardown and organization capabilities. If you aren\u0026rsquo;t already familiar with testify/suite, I would recommend you take a quick detour of the docs and examples before reading on.\nCase study: LinkedIn post scraping For this article I am going to go through an example of testing a stream processing application which calls the LinkedIn APIs to get organization posts and their lifetime metrics. I am going to attempt to show you the tests without the application code, because I do not believe showing the code brings much value. Just know that the code we are testing does the following:\nCalls the LinkedIn ListUGC endpoint to get a list of user generated content (posts) from the organization. Calls the EntityShareStatistics endpoint to request the lifetime metrics for each post less than a week old. Returns a list of the posts, with metrics (if they are retrievable). The test setup I begin all my tests with the same layout: The testify/suite structs and methods, mocks and fixture setup.\n// Constants to improve readability const ( Fail = false Succeed = true ) // Mocks is a collection of all the mocks used in the tests type Mocks struct { LinkedinClient mocks.LinkedinClient } // Fixtures is a grouping of entities interacted with in the tests type Fixtures struct { input *Input } // Expectations are a grouping of the results or expectations of the tests type Expectations struct { Output *Output Error error } // LinkedinCollectorTestSuite will test the collection of the LinkedIn post and metric collector type LinkedinCollectorTestSuite struct { suite.Suite mocks Mocks fix Fixtures expect Expectations collector *Collector } // Before every test, setup new mocks, fixtures and expectations func (s *LinkedinCollectorTestSuite) SetupTest() { s.mocks = Mocks{} s.fix = Fixtures{} s.expect = Expectations{} } func (s *LinkedinCollectorTestSuite) TearDownTest() { t := s.T() s.mocks.LinkedinClient.AssertExpectations(t) } There are a few things I want to highlight:\nThe Mocks{} structure is created with values, not pointers. This makes initializing all the mocks very easy because the testify/mocks do not need a constructor, their zero values are ready to use. I keep mocks, fixtures and expectations in their own respective structures. This helps readability and also allows you to define methods on them for further customization. I define a few constants such as Succeed and Fail for readability. This will become more clear later as we start to write the tests. Mock, fixture and expectation setup are done in the SetupTest() so that they are run before each test, providing fresh values and no artifacts from previous runs. The TearDownTest() method is used to assert the mock calls after each test. Once we have our initial skeleton setup, we can start writing tests for our code flows. I always start off testing the happy-path: the code flow where there are no errors and everything works as expected. The reason for this is two-fold:\nIt gives us a good context from which subsequent test cases can be built from. (Most test cases are just varying degrees in which the happy path becomes sad) The happy path allows us to validate that the business logic is correct when things are working as expected. To do this, we build all our fixtures and expectations based on this happy path in the SetupTest() method:\ntype Input struct { linkedinPageID string\t} type Output struct { Posts []*Post } // This would be a model defined by the application, but I am redefining it here for // clarity of reading the tests type Post struct { ID string Type string CreatedDate time.Time Metrics map[string]float64 } // Fixtures is a grouping of entities interacted with in the tests type Fixtures struct { linkedinPageID string linkedinAuthToken string textPost *Post videoPost *Post input *Input } // Before every test, setup new mocks, fixtures and expectations func (s *LinkedinCollectorTestSuite) SetupTest() { s.mocks = Mocks{} s.fix = Fixtures{ linkedinPageID: \u0026#34;test_page_id\u0026#34;, linkedinAuthToken: \u0026#34;mock_token\u0026#34;, input: \u0026amp;Input{linkedinPageID: \u0026#34;test_page_id\u0026#34;}, textPost: \u0026amp;Post{ ID: \u0026#34;1\u0026#34;, Type: \u0026#34;TEXT\u0026#34;, CreatedDate: time.Now().AddDate(0, 0, -1), // 1 day old Metrics: map[string]float64{\u0026#34;likes\u0026#34;: 1, \u0026#34;shares\u0026#34;: 1}} videoPost: \u0026amp;Post{ ID: \u0026#34;2\u0026#34;, Type: \u0026#34;VIDEO\u0026#34;, CreatedDate: time.Now().AddDate(0, 0, -3), // 3 days old Metrics: map[string]float64{\u0026#34;likes\u0026#34;: 2, \u0026#34;shares\u0026#34;: 2}} } s.expect = Expectations{ Output: \u0026amp;Output{Posts: []*Post{s.fix.textPost, s.fix.videoPost}} } } We also add a test case for the happy path:\nfunc (s *LinkedinCollectorTestSuite) HappyPath() { got, err := s.collector.Collect(s.fix.input) s.NoError(err) s.Equal(t.expect.Output, got) } Mock Parameterization Now our tests will not pass until we setup our mock calls. In the past, I have seen developers setup their mock calls in each test, often applying the same copy-paste code for a majority of the mocks, and tweaking just one. Do not do this. This leads to huge test files that are hard to refactor and tests that drown out the important aspects with irrelevant boilerplate. Instead, I strongly suggest having mock setup functions that can be parameterized to meet all the different demands of your individual tests. Here is one such example:\nfunc (s *LinkedinCollectorTestSuite) setupListUGCCall(succeed bool) { // mock.Anything is for the context.Context argument call := s.mocks.LinkedinClient.On(\u0026#34;ListUGC\u0026#34;, mock.Anything, s.fix.linkedinPageID, s.fix.linkedinAuthToken) if succeed { resp := \u0026amp;ListUGCResponse{ Posts: []*Post{s.fix.textPost, s.fix.videoPost} } call.Return(resp, nil) return } // Otherwise fail with an error and adjust expectations s.expect.Output = nil s.expect.Error = fmt.Error(\u0026#34;failed to call ListUGC endpoint\u0026#34;) call.Return(nil, s.expect.Error) } In the example above, we have added a single parameter, succeed, to alter the direction of the code flow. If succeed is false, then the mock is setup to make the LinkedIn API call fail, and we adjust our expectations accordingly. This is perhaps the simplest mock parameterization you can do, but it gives you an idea the power of parameterization. Testing the happy path and the code flow where the ListUGC endpoint fails becomes easy to implement and read:\nfunc (s *LinkedinCollectorTestSuite) HappyPath() { s.setupListUGCCall(Succeed) got, err := s.collector.Collect(s.fix.input) s.NoError(err) s.Equal(t.expect.Output, got) } func (s *LinkedinCollectorTestSuite) ListUGCFails() { s.setupListUGCCall(Fail) got, err := s.collector.Collect(s.fix.input) s.EqualError(err, \u0026#34;failed to call ListUGC endpoint\u0026#34;) s.Nil(got) } In situations where the error is immediately returned up the stack, testing this code path is only really beneficial for improving code coverage. Often times, you have more complicated error handling that you specifically want to test. This is where mock parameterization really shines.\nThe next mock call is for the EntityShareStatistics endpoint, which is called for each post and returns the post\u0026rsquo;s lifetime metrics. If the call to gather metrics fails, we want to carry on publishing the post without the metrics. This is how the mock setup method looks:\nfunc (s *LinkedinCollectorTestSuite) setupEntityShareStatisticsCall(succeed bool, mockPost *Post) { // mock.Anything is for the context.Context argument call := s.mocks.LinkedinClient.On(\u0026#34;EntityShareStatistics\u0026#34;, mock.Anything, mockPost.ID, s.fix.linkedinAuthToken) if succeed { resp := \u0026amp;EntityShareStatisticsResponse{ PostID: mockPost.ID, Metrics: mockPost.Metrics, } call.Return(resp, nil) return } // mockPost is referenced in the expectations, so modifying it here adjust the expectations // If we used a value instead of a pointer, we would need to modify the expectations directly. mockPost.Metrics = nil call.Return(nil, s.expect.Error) } and now our happy path becomes:\nfunc (s *LinkedinCollectorTestSuite) HappyPath() { s.setupListUGCCall(Succeed) s.setupEntityShareStatisticsCall(Succeed, s.fix.textPost) s.setupEntityShareStatisticsCall(Succeed, s.fix.videoPost) got, err := s.collector.Collect(s.fix.input) s.NoError(err) s.Equal(t.expect.Output, got) } The failure path is nearly the same, but instead we pass Fail to one of the setupEntityShareStatisticsCall:\nfunc (s *LinkedinCollectorTestSuite) EntityShareStatisticsFailsForOnePost() { s.setupListUGCCall(Succeed) s.setupEntityShareStatisticsCall(Fail, s.fix.textPost) s.setupEntityShareStatisticsCall(Succeed, s.fix.videoPost) got, err := s.collector.Collect(s.fix.input) s.NoError(err) s.Equal(t.expect.Output, got) } Testing Edge Cases We have an edge case in the EntityShareStatistics endpoint. The endpoint only lets you gather metrics for posts that are less than a certain age (for simplicity, let\u0026rsquo;s say 1 week). Calling the endpoint on a post older than a week would return an error and consume rate limits. To avoid issues, we want to skip these posts with a simple post age check. This is what a test for this edge case would look like:\nfunc (s *LinkedinCollectorTestSuite) SkipTooOldPost() { // older than one week --\u0026gt; No call to EntityShareStatistics endpoint s.fix.textPost.CreatedDate = time.Now().AddDate(0, 0, -8) // Since we do not call the EntityShareStatistics endpoint for this post, it will have no metrics s.fix.textPost.Metrics = nil s.setupListUGCCall(Succeed) s.setupEntityShareStatisticsCall(Succeed, s.fix.videoPost) got, err := s.collector.Collect(s.fix.input) s.NoError(err) s.Equal(t.expect.Output, got) } As you can see, we simply mutate the fixtures before the test runs and adjust the mock calls accordingly. For the mocks, all we needed to do was remove the EntityShareStatistics call for the textPost, which we made older than one week. Since the expectations uses a pointer to the post fixture, we just need to modify the fixture itself. If we chose to use a value instead of a pointer, then we would also need to update the expectations directly.\nFurther flexibility of the mock parameterization can be attained by using functional options. Functional options becomes more useful when your mock setup calls start having too many parameters and thus start having mulitple code paths. It is important to keep the mock setup calls simple and easy to read.\nAlternatives Table tests One testing paradigm that became popular in the Go community are table tests. Table tests are great for very simple, low complexity code, however they come with the tradeoff of reduced readability. Table tests excel in situations where there is no setup code, state or mocks/integrations and for when there are lots of edge cases based solely on varying inputs. In my opinion, they are not great for testing complex workflows.\nBehavior Driven Testing (DDD) Behavior Driven Testing is an evolution of Test Driven Development (TDD) where test cases and expectations are written in a natural language, such as English. It allows for business people to write test cases and developers implement them. I am a huge fan of Behavior Driven Testing as it leads to higher quality tests, coverage and a better understanding between both development and business teams.\nGo Convey and Ginkgo are two popular BDD style frameworks for the Go language. However, they do have a steeper learning curve than testify. These two libraries have a unique test execution strategy where the tests and sub-tests are executed by traversing the test tree in depth-first approach. Understanding this becomes even more important when you have tests that communicate with external dependencies like databases. Each iteration runs from the root of the tree down to a leaf, creating its own scope that is independent to other iterations. This execution strategy is very powerful because it eliminates a lot of fixture generation boilerplate, allowing you to test all edge cases efficiently and effectively.\nConclusion There are many different styles and testing frameworks available in Go. In this article I\u0026rsquo;ve talked about my own style that has serviced me well for testing complex workflows. If you decide to try it, I would love to hear your feedback.\n","ref":"/posts/golang_testing/"},{"title":"Advanced Go Error Handling Made Simple","date":"","description":"","body":" One of the most notorious design decisions of Go is how it approaches error handling. Go handles errors explicitly by returning them as values from any function that can possibly raise an error. To many, this way of error handling is uncomfortable at first. The tediousness of having every function not only return an optional error but also check the returned error from every function seemed to be too tedious.\nAfter working with the language for several years, I have come to love the way Go approaches error handling. Requiring the programmer explicitly think of all error paths and forcing them to handle them immediately leads to higher quality code and a deeper understanding of how their software can react. There are several high quality articles on the practical usages of standard library errors in Go, so I will just briefly touch upon the main concepts laid out in order to build some foundation for what is next.\nIn Go, errors are interfaces with a single method:\ntype error interface { Error() string } The standard library provides an easy way to create simple errors using the fmt package:\nuserID := \u0026#34;23\u0026#34; err := fmt.Errorf(\u0026#34;user with id = %s already exists\u0026#34;, userID) // \u0026#34;user with id 23 already exists\u0026#34; You can also \u0026ldquo;wrap\u0026rdquo; an existing errors with additional information. The underlying, or \u0026ldquo;wrapped\u0026rdquo; error, can still be extracted using the errors.As() or errors.Is() functions. Note the use of %w instead of %s to indicate wrapping:\nerrWrapped := fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) // \u0026#34;failed to create user: user with id 23 already exists\u0026#34; Both these methods return a very primitive implementation of the error interface, which essentially only contains a string and the wrapped error (if it exists). This is in-line with Go\u0026rsquo;s light-weight, less-is-more philosophy. On the other hand, there is a lot lacking from these basic errors. Fortunately, because error is an interface, we are free to define our own implementation.\nIn this article, we will introduce a new error implementation from the simplerr package, with a main goal of reducing boilerplate and increasing code legibility for practical and common error handling scenarios. The primary design decision of the package was to have error handling logic confined to middleware layers and to propagate any input parameters for the logic on the error itself.\nBut before we jump into it, I want to talk about what errors are, are not, and some use-cases in which the standard library implementation is insufficient.\nErrors are meant to be handled The main purpose of errors are to convey when something has gone wrong and to provide enough information so that the software or the client can handle it. There are an infinite number of ways software can error, so to assist with error handling on the client side, specifications such as HTTP and gRPC define categories of errors. Developers using these protocols need to translate errors raised in the application to the specification\u0026rsquo;s error codes.\nTable 1: Table of some HTTP status codes and equivalent gRPC codes. [Reference]\nHTTP Status Code gRPC Status Code 400 Bad Request 13 Internal 401 Unauthorized 16 Unauthenticated 403 Forbidden 7 Permission Denied 404 Not Found 12 Unimplemented 429 Too Many Requests 14 Unavailable 502 Bad Gateway 14 Unavailable 503 Service Unavailable 14 Unavailable 504 Gateway Timeout 14 Unavailable All other codes 2 Unknown Often times I see this translation being done manually over each segment of code that returns an error in the API layer.\nProblem 1: Error translation to HTTP/gRPC specifications must be done manually for every function that returns an error.\nContextual Errors and Structured Loggers Structured logging is one of the best ways to build observability into your system. By attaching key-value pairs to log statements, one can more readily parse, filter and query specific log messages based on the context of the code at the time. Observability stacks such as ELK or Sumologic can provide a powerful way to aggregate, index and search through millions of logs messages.\nFigure 1: Example of a structured log in JSON formatting.\n{ \u0026#34;level\u0026#34;:\u0026#34;info\u0026#34;, \u0026#34;time\u0026#34;:\u0026#34;2022-02-19T10:16:42Z\u0026#34;, \u0026#34;caller\u0026#34;:\u0026#34;eventgenerator/processor.go:90\u0026#34;, \u0026#34;msg\u0026#34;:\u0026#34;Generating event\u0026#34;, \u0026#34;shard\u0026#34;:1, \u0026#34;operation_type\u0026#34;:\u0026#34;update\u0026#34;, \u0026#34;cluster_time\u0026#34;:\u0026#34;2022-02-19T10:16:42Z\u0026#34;, \u0026#34;stream_lag\u0026#34;:\u0026#34;561.806189ms\u0026#34;, \u0026#34;content_id\u0026#34;:\u0026#34;6INvoQTXzZM\u0026#34;, \u0026#34;event_name\u0026#34;:\u0026#34;content-modified\u0026#34; } In the majority of cases, you should be logging errors and providing as much contextual information as possible in order to make debugging as smooth as possible[1]. The standard library errors do not provide a way to pass this contextual information in a way that can easily be extracted and passed to structured loggers. This often results in errors being logged manually at the return-site of the error in order to transfer context to the logs. This leads to bloated error handling logic which distracts from the intent of the code.\nProblem 2: Logging of errors is often a manual process due to a disjointed interaction between error and logging packages.\nIf we were able to attach key-value information to the error, we may be able to automate the logging of errors in a middleware layer. Unfortunately, the standard library http package does not have an error return argument to HTTP handlers, which makes writing error logging middleware difficult. Fortunately simplerr provides an alternative Handler interface and adapters in order to return errors. The gRPC framework, on the other hand, does return an error and makes this straight forward with the use of an interceptor.\nDecoupling errors from different layers is tedious In order to decouple layers of in our software, we need to prevent leaking of implementation details via errors that need to be detected in above layers. For example, in order to detect that a unique constraint has been violated when using MongoDB in the persistence layer, we need to detect a particular error raised by the mongo library:\n// IsDuplicateKeyError checks that the error is a mongo duplicate key error func IsDuplicateKeyError(err error) bool { const mongoDuplicateKeyErrorCode = 11000 if mongoErr, ok := err.(mongo.WriteException); ok { for _, writeErr := range mongoErr.WriteErrors { if writeErr.Code == mongoDuplicateKeyErrorCode { return true } } } return false } While this function is fine to be used inside the persistence layer, we should not use it inside the application layer with which it interfaces due to the direct reference to the mongo package. One method of abstracting the mongo package is to have the persistence layer define it\u0026rsquo;s own AlreadyExistsError that is raised instead whenever the mongo library returns an error from writing to a duplicate key. This allows us to change the storage library used in the persistence layer away from MongoDB without breaking changing any other software layers.\n// Define an error in the persistence layer package that we can return instead of the mongo.WriteException type UserAlreadyExistsError struct { email string } // Error implements the error interface type (e *UserAlreadyExistsError) Error() string { return fmt.Sprintf(\u0026#34;user already exists with email \u0026#39;%s\u0026#39;\u0026#34;, e.email) } // Create a user by it\u0026#39;s email. The email is the unique key for looking up users. func (s *Database) CreateUser(ctx context.Context, email string) (string, error) { user := User{email: email} result, err := s.mongodb.InsertOne(ctx, user) // Check if the error was from mongo.WriteException and return UserAlreadyExistsError instead if IsDuplicateKeyError(err) { return \u0026amp;UserAlreadyExistsError{email: email} } if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } return result.Hex(), nil } While this is not a huge amount of added code, this work compounds if you have several separate persistence packages or different data storage libraries that each need their own abstracting. To make things worse, if you are using a transport layer, you will need to do the same detection and translation yet again in order to return the proper error statuses defined in the transport specification.\nfunc (s *Server) CreateUser(resp http.ResponseWriter, req *http.Request) { // extract email from request... err := s.db.CreateUser(email) if err != nil { statusCode := http.StatusInternalServerError // If the error was from an UserAlreadyExistsError, change the status code to BadRequest var alreadyExistsErr storage.UserAlreadyExistsError if errors.As(err, \u0026amp;alreadyExistsErr) { statusCode = http.StatusBadRequest } resp.WriteHeader(statusCode) return } resp.WriteHeader(http.StatusOK) } If we take a deeper look into the duplicate key error, we may be able to capture the essence of what this error signals and fit it into a category much like the ones used by gRPC and HTTP specifications.\nProblem 3: Abstracting and propagating errors from third party dependencies is verbose and manually intensive.\nNot all errors are bad In Go, errors are sometimes intentionally returned as a way of signaling a certain code path or expected state is reached. For example, the io package defines an io.EOF sentinel error to signal that the reader has reached the end of a byte stream. In this case, io.EOF isn\u0026rsquo;t a true error, but instead a convenient way to indicate a special case in the programming flow. Let\u0026rsquo;s call these types of errors benign errors.\nBenign errors can often be difficult to work with. Let\u0026rsquo;s look at an example of an API which returns some resource, such as a Settings object for a particular optional feature. Callers of this API can look for response codes 404 (HTTP) or 5 (gRPC) to determine if the user has enabled this feature. On the server side, the SQL-based storage layer may return a sql.ErrNoRows error for such calls. If we were to log each of these errors at the ERROR level we would be flooding the logs with benign errors. In this situation, the real error is on the client side, depending on whether the caller is expecting the user to have the feature or not. The client may be simply checking whether the user has the feature enabled, in which case, it is also a benign error on the caller side. On the server, we should be able to detect these kind of errors and choose not to log them as errors, yet still return them as errors to the client. This becomes particularly difficult when error translation and logging are done within middleware.\nProblem 4: Handling of benign errors on the server side cannot be done from within middleware.\nNo control over how errors are retried Some errors are transient in nature. A hiccup in the network or temporarily unavailable service may cause a request, that failed a moment ago, to succeed by just retrying the request. In these cases, it can be useful to retry the request with an exponential backoff, in hopes of eventually succeeding. However, not all errors should be retried[3] and there is not a standard way to convey this information to upstream callers.\nProblem 5: Standard library errors have no way to convey additional information on how to handle the error.\nDesigning a Better Error Given that the error interface is so small, we can implement our own implementation that to alleviate the aforementioned problems. The rest of the article will be used to introduce you to the simplerr package and show you how it can help you implement better error handling.\nSimplerr defines a single error implementation, called the SimpleError. The SimpleError has several traits that make it very flexible for describing and handling errors. Let\u0026rsquo;s look at how the SimpleError solves each of the problems previously outlined.\nProblem 1: Error translation to HTTP/gRPC specifications must be done manually for every function that returns an error. Simplerr defines a set of standard error codes for common errors that occur in software. Codes such as NotFound or PermissionDenied are self explanatory and have analogs in HTTP and gRPC specifications. If the list of standard codes does not fit your error, you can globally register your own error codes with the package. Errors can then be handled by their error code rather than type or value. This allows us to label and detect errors in a more human-readable and dependency-agnostic way.\nuserID := 123 companyID := 456 err := simplerr.New(\u0026#34;user %d does not exist in company %d\u0026#34;, userID, companyID). Code(CodeNotFound) These error codes can also be translated automatically to HTTP / gRPC specifications (see ecosystem/http and ecosystem/grpc packages). In the case of gRPC, an interceptor (middleware) allows us to handle this translation automatically via an interceptor.\nfunc main() { // Get the default mapping provided by simplerr m := simplerr.DefaultMapping() // Add another mapping from simplerr code to gRPC code m[simplerr.CodeMalformedRequest] = codes.InvalidArgument // Create the interceptor by providing the mapping interceptor := simplerr.TranslateErrorCode(m) // Apply the interceptor to the gRPC server... } Problem 2: Logging of errors is often a manual process due to a disjointed interaction between error and logging packages. With SimpleError, you can attach auxiliary information to the error as key-value pairs, using the Aux() and AuxMap() methods. These fields can be extracted and used with structured loggers in a middleware layer. This eliminates the need to choose between manually logging errors at the point at which they are raised or surrendering structured logging fields.\nuserID := 123 companyID := 456 err := simplerr.New(\u0026#34;user %d does not exist in company %d\u0026#34;, userID, companyID). Code(CodeNotFound). Aux(\u0026#34;user_id\u0026#34;, userID, \u0026#34;company_id\u0026#34;, companyID) Retrieving the fields should be done with the ExtractAuxiliary() function so that auxiliary fields from each wrapped error in the chain are retrieved as well.\nSimpleError does not have logging integration due to the difficulty of defining a logging interface that works well with the variety of loggers out there. Although adapters could always be written to satisfy the interface, it would involve extra steps and does not follow the philosophy of \u0026ldquo;keeping it simple\u0026rdquo;. Fortunately, integrating logging (and more particularly, structured logging) is simple with the use of custom attributes.\nMuch like the context package, SimpleError allows you to attach arbitrary key-value information onto the error. We can use this feature to attach the structured logger at the site at which the error is raised, capturing all the scoped logging fields with it.\nThe following is an example of attaching a zap logger to a raised error.\n// Define a custom type so we don\u0026#39;t get naming collisions for value == 1 type ErrorAttribute int // Define a specific key for the attribute const LoggerAttr = ErrorAttribute(1) // Attach the `LoggerAttr` attribute on the error serr := simplerr.New(\u0026#34;user with that email already exists\u0026#34;). Code(CodeConstraintViolated). Attr(LoggerAttr, logger) We can then write middleware which extracts this logger (if it exists) from the error to log the error.\nfunc ErrorHandlingMiddleware(....) (...) { if err != nil { // Get the logger by using the LoggerAttr key scopedLogger, ok = simplerr.GetAttribute(err, LoggerAttr).(*zap.Logger) if ok { auxFields := simplerr.ExtractAuxiliary(err) // log the error with the scoped logger and the auxiliary fields scopedLogger.Error(...) } else { // log the error with the standard logger log.Error(...) } } } Problem 3: Abstracting and propagating errors from third party dependencies is verbose and manually intensive. By using error codes rather than sentinel errors or custom error types, we can greatly simplify how we abstract errors from third parties. We can detect and convert errors, as we would traditionally do, but instead of defining a custom error, we return a SimpleError.\n// IsDuplicateKeyError checks that the error is a mongo duplicate key error func IsDuplicateKeyError(err error) error { const mongoDuplicateKeyErrorCode = 11000 if mongoErr, ok := err.(mongo.WriteException); ok { for _, writeErr := range mongoErr.WriteErrors { if writeErr.Code == mongoDuplicateKeyErrorCode { // Return a simpleerr.SimpleError here return simplerr.Wrap(err).Code(simplerr.CodeConstraintViolated) } } } return nil } The previous example of creating a user then looks like this, allowing us to use the ecosystem/http package to convert errors directly to status codes on the *http.Response object.\n// Create a user by it\u0026#39;s email. The email is the unique key for looking up users. func (s *Database) CreateUser(ctx context.Context, email string) (string, error) { user := User{email: email} result, err := s.mongodb.InsertOne(ctx, user) // Check if the error was from mongo.WriteException and return the SimpleError with an attached message if serr := IsDuplicateKeyError(err); serr != nil { return fmt.Errorf(\u0026#34;user already exists with email \u0026#39;%s\u0026#39;: %w\u0026#34;, e.email, err)) } if err != nil { return \u0026#34;\u0026#34;, fmt.Errorf(\u0026#34;failed to create user: %w\u0026#34;, err) } return result.Hex(), nil } func (s *Server) CreateUser(resp http.ResponseWriter, req *http.Request) { // extract email from request... err := s.db.CreateUser(email) if err != nil { // SetStatus will attempt to translate the SimpleError to a status code on the *http.Response, if // it cannot find a translation, it defaults to 500 (InternalServerError) simplehttp.SetStatus(resp, err) return } resp.WriteHeader(http.StatusOK) } Using the alternative Handler interface defined in simplerr/ecosystem/http when writing HTTP handlers makes it even easier by not having to handle the error at all. The handlers will automatically write the response status depending on the returned error.\nAn analogous approach can be done for gRPC servers with the ecosystem/grpc package. This time it is even easier to convert SimpleErrors to response codes through an interceptor (middleware). In both http and gRPC, the error translation mapping can be customized.\nProblem 4: Handling of benign errors on the server side cannot be done from within middleware. SimpleErrors can be marked as silent or benign so that logging middleware can handle them differently. Benign errors can optionally add a reason why they are considered benign. This information is useful to have in the log when it comes time to debug. How you decide to handle silent or benign errors is ultimately up to you, however it is recommended that silent errors not be logged at all, and benign errors be logged at a less severe level such as DEBUG or INFO.\nif err != nil { // Check if the errror is a SimpleError if serr := simplerr.As(err); serr != nil { if serr.IsSilent() { // do not log silent errors return } // if the error is benign, log as INFO if reason, benign := serr.IsBenign(); benign { log.Info(....) return } } // log error at ERROR level log.Error(....) } Problem 5: Standard library errors have no way to convey additional information on how to handle the error Assigning additional attributes to errors can be done in a similar way to the context package and the context.WithValues() function. The following is an example of attaching an attribute to an error which indicates that this error should not be retried.\n// Define a custom type so we don\u0026#39;t get naming collisions for value == 1 type ErrorAttribute int // Define a specific key for the attribute const NotRetryable = ErrorAttribute(1) // Attach the `NotRetryable` attribute on the error serr := simplerr.New(\u0026#34;user with that email already exists\u0026#34;). Code(CodeConstraintViolated). Attr(NotRetryable, true) // Get the value of the NotRetryable attribute doNotRetry := simplerr.GetAttribute(err, NotRetryable).(bool) // doNotRetry == true Conclusion The error interface in Go allows us to define alternative implementations for errors. While the standard library errors are sufficient, they can be improved to better fit our workflow and style. The Simplerr package is just one way to implement a custom error. It solves many of the issues that I had encountered while developing Go services and APIs. Hopefully it can help you too.\nFootnotes [1] Be careful who will be receiving the errors you are returning. If it is just software that you own, you can be more transparent about the root cause of the issue. However, if you are exposing your API publicly, you do not want to give a potentially malicious user implementation details of your system.\n[2] The error interface was intentionally kept small in order to be easily adopted. Forcing thing such as key-value pairs into the error would be enforcing too much on the user who may not care about structured logging (for example, CLI developers).\n[3] Not all errors should be retried. Make sure your request is idempotent so that you are not causing more problems for yourself.\n","ref":"/posts/richer_golang_errors/"},{"title":"Defusing an ElasticSearch Mapping Explosion with Slots","date":"","description":"","body":" ElasticSearch (and similarly OpenSearch[1]) is a popular OLAP database that allows you to quickly search and aggregate your data in a rich and powerful way. It is a mature storage technology build on top of Apache Lucene that has been used to back many online storefronts and analytical processing products around the world. Under the hood, ElasticSearch uses Lucene to index each field in the document so that queries can be executed efficiently.\nIn order to provide rich searching capabilities, ElasticSearch creates indices for each field it receives. The type of index created is determined by the data type of the field. Possible data types include numeric types such as integer, float and double as well as two string data types, keyword and text. The list of all indexed fields (and how they are indexed) are stored in what is called an index mapping. By default, ElasticSearch will create a mapping entry when a new field is encountered by deducing it\u0026rsquo;s data type[2]. This is called dynamic mapping.\nThe problem with dynamic mapping is that once ElasticSearch introduces a field to the mapping, it can never be removed. The index mapping also consumes cluster memory and must be replicated to all nodes. If you are not careful, leaving dynamic mapping enabled can lead to very large index mappings and slow down your cluster. This is called a mapping explosion. For this reason, it is recommended to disable dynamic mapping and enable it sparingly only for objects that you can trust have a finite number of fields[3].\nIn many cases this is not an issue and it is not restrictive to disable dynamic mapping. However, in some cases a user may determine the name of a field. In such situations, one must come up with a strategy to mitigate the risk of an eventual mapping explosion. The rest of this article talks about how we mitigated a mapping explosion in HootSuite\u0026rsquo;s analytics products.\nThe problem context At Hootsuite, we use ElasticSearch to store social media content and their associated metrics such as likes, impressions shares and comments. With ElasticSearch, customers can search for content across all their social networks in a multitude of ways. They can also aggregate metric values from their content over a time range to give them a view of their social marketing performance. These metrics are retrieved from the social network APIs and stored in separate fields in ElasticSearch under a metrics object. These fields have the same name as the metric and are common among all users of the social network. Since there are a finite number of metrics, there is no risk of a mapping explosion. However, we also collect metrics from offsite attributions such as Google and Adobe Analytics. On these platforms, users can name and define their own metrics. These are the metrics we need to be careful about. Each customer now has the ability to permanently increase our mapping size by creating a unique metric name. The problem is even more serious when you consider that these mapping entries will be around long after a customer has stopped using that metric.\nDisabling these fields in the mapping is not an option. We needed to devise a way to still provide the search and aggregation functionality that is so critical to our product functionality, yet also have a stable long tem solution. To overcome this, we came up with a system we called Slots.\nDefusing a mapping explosion With the slots approach, we gain control of unbounded mapping growth by predefining a fixed number of fields (the slots) and translating them in the application code to the user-defined values. We call the application-level translation from metric to slot the slot-mapping.\nFigure 1: Translation of metric names into slots before writing to ElasticSearch. Each customer gets it\u0026rsquo;s own set of slots.\nEach user will have their own slot-mapping, meaning that the metric referred to by each slot will be different for each user. The number of slots we need to allocate needs to be, at a minimum, equal to the amount that the customer with the highest number of custom-named metrics has[4]. From looking at our own data, the customer with the largest amount of custom metrics had about 200 metrics. To be safe, we allocated 1000 slots.\nTrade offs The slots approach is not without it own caveats. For one, it makes debugging and reading data more difficult. Now if we inspect our data with third party tools such as Kibana, rather than seeing a field named custom_metric_c, we see slot_3. We need to refer back to the slot-mapping to determine which slot refers to the metric we are looking for.\nSecond, we must be careful that there are no requests for data that combine documents that use different slot-mappings. You cannot aggregate slot_2 across data from two different customers because the metric slot_2 refers to will be different. Luckily, aggregating across customers is not a valid business use-case and in fact goes again our multi-tenant principles.\nThird, a customer may eventually hit their 1000 slot limit by renaming or adding new metrics that they use temporarily. This has yet to happen but there are a few avenues for when it possibly does. We can reclaim some of the slots that are no longer in use by going through data and removing the values from documents. Alternatively, we can simply increase the maximum slots.\nDefining the slot mapping Each metric, whether it be from the social network or user defined, has a unique ID and some metadata associated with it called the Metric Definition. At first it was tempting to store the slot number on the metric definition itself, however we decided against it because slots are an implementation detail of the fact that we use ElasticSearch as a storage backend. By exposing the slots outside of the service performing the translation to/from metric name to slot, we are weakening our abstraction.\nInstead, we had the services that write to ElasticSearch each define their own slot-mappings. The slot mapping is defined the first time the service writes the metric to ElasticSearch. The flow requires us to keep a counter for the current number of slots for a user and also the relationship between slot and metric name.\nTable: slot_counters\nuser_id n_slots 1 2 2 4 3 0 Table 1: The slot_counters table keeps track of the number of slots each user has used.\nTable: slot_mappings\nuser_id slot metric 1 1 visits 1 2 bounces 2 1 website_visits 2 2 website_pageviews 2 3 ecommerce_revenue 2 4 goal_values Table 2: The slot_mappings table contains the relationships between slot and metric for each user. It has a unique constraint on combinations of (user_id, slot).\nWhen a new metric is written, the counter is incremented and a new slot_mapping row is written. The slot_mapping table must also have a unique compound index on the user_id and slot columns. This unique index prevents potential race conditions from multiple API requests trying to grab the next slot number at the same time. In the case of a race condition, one of the attempts to write to the slot_mapping table will fail on the unique-constraint and we can roll back the transaction and simply retry again.\nFigure 2: A flow diagram of how using a transaction, a unique constraint on the slot_mappings table and a retry mechanism keeps concurrent requests from creating the same slot mapping.\nBefore writing to ElasticSearch, all metric names are converted to their slot numbers. When reading or aggregating metrics, the slot names retrieved from ElasticSearch are translated back to the original metric names with the same slot_mapping table. This whole process is completely transparent to the user of the API.\nA small LRU caching layer is added around the metric slot-mapping values to reduce the number of times we need to go to the database. Since these values are static and small, this ends up working very well.\nConclusion ElasticSearch / OpenSearch works well for indexing complex data schemas but has limitations when the the number of fields is unbounded. Using application-level translation with the Slots approach, we can multiplex a fixed number of fields to represent a larger number of fields and prevent a mapping explosion. The trade off is an overall increase in complexity and reduction to the ease of debugging. The performance impact of adding an additional translation layer is negligible with the help of an LRU cache.\nFootnotes [1] In 2021, Elastic.co, the company that owns the license to ElasticSearch, changed their license to be more restrictive and in conflict of open source values. In response, Amazon created a fork of ElasticSearch 7.10.2 named OpenSearch which continues the Apache 2.0 license.\n[2] ElasticSearch prevents you from shooting yourself in the foot too much by setting an upper limit on the number of fields in a mapping. As of writing this, the default is 1000. This limit can be increased at any time.\n[3] Dynamic mapping has another problem, it will set the field settings to general purpose presets. If you want more control of your fields, such as defining a custom analyzer, you should define the field mappings explicitly.\n[4] Generally speaking, having hundreds or even thousands of fields in an ElasticSearch mapping is not as much of an issue as the unbounded growth of the mapping.\n","ref":"/posts/elasticsearch_slots/"},{"title":"Reducing System Load With Event Folding","date":"","description":"","body":" One of the prevailing forms of communication in modern microservice architectures is asynchronous messaging. This is the backbone of the event-driven architecture model. In this model, services send messages to a message broker, which then distributes (publishes) the messages to interested (subscribed) clients. A message can be used to describe an event in the system, such as the creation or update of an entity. This allows you to loosely couple different components of your system by having them publish or subscribe to events that they care about.\nBut what happens if there is a subscriber that wants to react to a group of events as an aggregate? In other words, what if you wanted to do something only after all the update events stop being published, rather than reacting to each and every one as they come in?\nLet\u0026rsquo;s look at a concrete example.\nAggregating Social Media Metrics Let\u0026rsquo;s say you are building a system that collects metrics on your social media posts. Your system needs to provide both an individual view of post performance and view of posts\u0026rsquo; performance as a whole. Let\u0026rsquo;s call metrics on individual posts post-level metrics and metrics on your posts as a whole account-level metrics. To keep things simple, let\u0026rsquo;s assume all post-level metrics can be aggregated using a sum:\n$M_{account} = \\sum_{i=0}^{N_{posts}} M_{post_{i}}$\nFrom the equation above, it is clear that account-level metrics depend on the post-level metrics. Every time a metric on a post changes, we will need to recompute this sum[1]. In an event-driven architecture, you may have a service that performs this aggregation by listening to post-metric-updated events. If many of these events are produced within a short period of time, the service will be computing this sum many times over, with only the last computation being valuable (all other computations would produce a stale value). What we would really like to do in this situation is detect when the last post-metric-updated event comes in and only then compute the sum. But how do we know which event is the last?\nDetermining which event is last is actually a difficult problem and can\u0026rsquo;t be solved in a general context. The concept of \u0026ldquo;last\u0026rdquo; is application specific. In our example, a post can have it\u0026rsquo;s metrics updated at any time. If we are okay with some degree of staleness to our account metrics, we can implement a delayed computation that occurs only after we stop receiving events for a certain amount of time.\nThe Ideal Event Ideally, what we would like to have is to have a slightly different event than our post-metric-updated event. What we really want is a post-metric-last-updated-X-minutes-ago event, where X is some acceptable delay. If each post-metric-updated event contains a list of all the updated metrics, then the post-metric-last-updated-X-minutes-ago event should contain the superset of all the metrics that were updated. Let\u0026rsquo;s define the time duration, X, the folding window because it is the minimum duration of time at which similar events will be folded into a single event.\nBut how do we produce such an event?\nWe will need some code that will do the following for each account:\nListen for post-metric-updated events Keep a super-set of all the updated metrics in the events Keep track of the last time a post-metric-updated event comes in Produce a post-metric-last-updated-X-minutes-ago when we don\u0026rsquo;t receive an event for X minutes This turns out to be fairly simple to implement with the use of Redis sorted-sets.\nSorted Set A sorted set is a unique collection of strings that have an associated score. The score is used to sort the entries within the set. For example, a sorted set would be a perfect data structure for storing a user scoreboard:\nRank Name Highscore 1 Sharon 1050 2 Abdul 800 3 Calvin 780 4 Pirakalan 660 5 Nirav 600 We can insert the data above into the sorted set in any order and it will maintain the entries in the order of high-score.\n\u0026gt;\u0026gt; ZADD highscores 780 Calvin \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 1050 Sharon \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 660 Pirakalan \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 600 Nirav \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 800 Abdul \u0026gt;\u0026gt; (integer) 1 It is then very simple to ask the sorted set for the top N results with ZREVRANGE. (You could also get the results sorted by ascending order by using ZRANGE)\n\u0026gt;\u0026gt; ZREVRANGE highscores 0 5 WITHSCORES 1) \u0026#34;Sharon\u0026#34; 2) 1050 3) \u0026#34;Abdul\u0026#34; 4) 800 5) \u0026#34;Calvin\u0026#34; 6) 780 7) \u0026#34;Pirakalan\u0026#34; 8) 660 9) \u0026#34;Nirav\u0026#34; 10) 600 Or get values within a range of scores with ZRANGEBYSCORE.\n\u0026gt;\u0026gt; ZRANGEBYSCORE highscores 700 800 WITHSCORES 1) \u0026#34;Abdul\u0026#34; 2) 800 3) \u0026#34;Calvin\u0026#34; 4) 780 And of course you can delete keys from the sorted-set with ZREM.\n\u0026gt;\u0026gt; ZREM highscores Abdul (integer) 1 Folding Events Since the sorted set can only have unique values, we can use it to \u0026ldquo;fold\u0026rdquo; a group of events into a single event. In order to do that, we need to remove unique information from the events such that they serialize to the same string value. We can use the time of the event as the score in the sorted-set. Each time an event is added, we update the score, thus keeping track of the last event in the group. Our sorted-set then holds a time-ordered collection of events, where each element represents a folded view of events from each group. We can then regularly query the sorted-set with ZRANGEBYSCORE to get values with timestamps (scores) before time.Now()-X. These values are then published as post-metric-last-updated-X-minutes-ago events and removed from the sorted set[2].\nBack to our example, the post-metric-updated event has the following schema:\n// PostMetricUpdatedEvent is published when metrics change on a post. It contains the changed metric values. type PostMetricUpdatedEvent struct { PostID string `json:\u0026#34;id\u0026#34;` AccountID string `json:\u0026#34;account_id\u0026#34;` UpdatedMetrics map[string]float64 `json:\u0026#34;metrics\u0026#34;` } And we have the following events being published, shown below in their JSON serialized form:\n{\u0026#34;id\u0026#34;: \u0026#34;post_1\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 10, \u0026#34;shares\u0026#34;: 5}} {\u0026#34;id\u0026#34;: \u0026#34;post_2\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;comments\u0026#34;: 25, \u0026#34;impressions\u0026#34;: 16}} {\u0026#34;id\u0026#34;: \u0026#34;post_3\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 5, \u0026#34;shares\u0026#34;: 2}} {\u0026#34;id\u0026#34;: \u0026#34;post_4\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;comments\u0026#34;: 33, \u0026#34;impressions\u0026#34;: 8}} {\u0026#34;id\u0026#34;: \u0026#34;post_5\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 12, \u0026#34;shares\u0026#34;: 15}} {\u0026#34;id\u0026#34;: \u0026#34;post_6\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 3, \u0026#34;shares\u0026#34;: 1}} These 6 events span two unique accounts (account_1 and account_2) and should therefore ideally create two separate post-metric-last-updated-X-minutes-ago events. Remember, a sorted set stores unique strings, so we cannot just insert these JSON strings into the sorted-set as is. If we did, they would be stored as 6 different entries in the set. We need to identify the group key for all the events that we want to fold. The group key should remove all uniquely identifying information in the event such that events within the same group have the same serialized value. In our example, the group key would be just the account_id field, since we just want one event per account:\n{\u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;} {\u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;} But wait\u0026hellip; We just lost a bunch of information from our events! That\u0026rsquo;s right. I never said this folding was lossless! In some cases this may be acceptable, you may be able to gather that information elsewhere, say from an API call, or it may not be relevant for what you are trying to do on the subscriber side.\nIn cases where you do need that information, we can store it separately in a database as we are inserting values into the sorted-set. When querying the sorted-set for events to publish, we can \u0026ldquo;unfold\u0026rdquo; the event by enriching it with the information we stored in the database.\nIn our example, we need a list of metrics that have changed so that we know which metrics to aggregate. We can easily keep track of these metrics in a regular Redis set. When we query the sorted-set to get the folded event older than X, we also grab the metrics that have changed from the regular set. We can then assemble the post-metric-last-updated-X-minutes-ago event:\ntype PostMetricUpdatedEvent struct { AccountID string `json:\u0026#34;account_id\u0026#34;` UpdatedMetrics []string `json:\u0026#34;metrics\u0026#34;` } which in our example looks like:\n{\u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;likes\u0026#34;, \u0026#34;shares\u0026#34;, \u0026#34;comments\u0026#34;, \u0026#34;impressions\u0026#34;]} {\u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;likes\u0026#34;, \u0026#34;shares\u0026#34;]} The service performing the aggregation can then subscribe to these events and be notified only once metrics have stopped being updated. We can rest assured that we will compute the value at most once in the X minutes since the last time a post metric was updated.\nFolding Ratio To get a feel for how long we should set our folding window to be, we can calculate the folding ratio, ${F}_{R}$:\n${F}_{R}=\\frac{N_F}{N_E}$\nwhere $N_E$ = Number of fold-eligible events and $N_F$ = Number of actually folded events.\nWhen inserting into a Redis sorted-set with the ZADD operator, Redis will tell you how many new elements were added to the sorted-set (score updates excluded). Assuming we are inserting one event at a time, the number of folded events is measured by how many times the ZADD operation returns 0 and the number of new events is measured by how many times ZADD returns 1. With this in mind, we can measure the following values:\n$N_E= N_{Total} - Count(ZADD == 1)$\n$N_F$ = Count(ZADD == 0)\n$F_R = \\frac{Count(ZADD == 0)}{N_{Total} - Count(ZADD == 1)}$\nwhere $N_{Total}$ is the total number of events.\nBut there is a caveat; we are also popping elements off of the sorted set in order to publish the folded event. If the folding window is too short for the timing of our incoming events, we are going to be removing an element from the sorted set just before adding a new event that would have otherwise been folded. In this situation, we will be over measuring our number of new events and under measuring the number of folded events. Another way to look at this is that the value of ZADD only acts as a proxy for the folded count from Redis\u0026rsquo; perspective.\nBelow is an example of what that could look like:\n{\u0026#34;id\u0026#34;: \u0026#34;post_1\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 10, \u0026#34;shares\u0026#34;: 5}} -\u0026gt; ZADD Returns 1 (new) {\u0026#34;id\u0026#34;: \u0026#34;post_2\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 25, \u0026#34;shares\u0026#34;: 16}} -\u0026gt; ZADD Returns 0 (folded) {\u0026#34;id\u0026#34;: \u0026#34;post_3\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 5, \u0026#34;shares\u0026#34;: 2}} -\u0026gt; ZADD Returns 0 (folded) Publish folded event {\u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;} {\u0026#34;id\u0026#34;: \u0026#34;post_4\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 33, \u0026#34;shares\u0026#34;: 8}} -\u0026gt; ZADD Returns 1 (fake new) {\u0026#34;id\u0026#34;: \u0026#34;post_5\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 12, \u0026#34;shares\u0026#34;: 15}} -\u0026gt; ZADD Returns 0 (new) Publish folded event {\u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;} {\u0026#34;id\u0026#34;: \u0026#34;post_6\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 3, \u0026#34;shares\u0026#34;: 1}} -\u0026gt; ZADD Returns 1 (fake new) If your system is processing a large number of folded events ($N_{Total} \\gg Count(ZADD == 1)$) and your folding window is reasonable[3], then the error becomes negligible and the folding ratio simplifies further:\n${F}_{R}\\approx\\frac{Count(ZADD == 0)}{N_{Total}}$\nIf it is really important that you have the most optimal folding ratio, you will need to do some offline analysis such as looking at historical events within a time range. In this case, you will actually know the number of events that should be folded. In most cases, the timing of your events will be irregularly spaced and you will never consistently achieve 100% folding. Even then, any non-zero folding is still cutting the amount of work your system is doing.\nThe example we have been using throughout this article is a real world scenario. In production we see anywhere between 85% and 98% folding over hundreds of thousands of events per day. Without event folding, we would have overloaded our database a long time ago and be forced to drop our event-driven approach in favour of something more traditional such as scheduled cron jobs.\nPlacement of the Event Folder The event folder can be placed on either the publisher side or on the consumer side. Where you decide to place the event folder ultimately comes down to how many consumers want folded events. By folding on the publisher side, you can reduce complexity for consumers downstream. However, if only one consumer cares about folding events, it may be cleaner to place the event folder on the consumer side and not have to make any changes at the producer. If you place the event folder on the publisher side, there is nothing wrong with publishing both folded and unfolded events, just be sure to place them on separate topics to prevent confusion.\nAnother Use-case: Database Syncing Another use-case where the event folder is highly effective is during database syncing. In a CQRS pattern, we can separate database technologies used for writing and reading by syncing all writes to the read database. Some databases such as ElasticSearch perform poorly for document updates[4]. Using an event folder we can reduce the number of writes to ElasticSearch, significantly reducing the number of deleted documents that need to be garbage collected.\nA Powerful Tool The event folder can be a powerful tool in your system\u0026rsquo;s architecture. It can be the difference in an event driven system\u0026rsquo;s viability. The event folder I have outlined in this article is designed around transient spikes in events. If your events are not transient, you may find that your folded events may never be published due to a constantly resetting folding window. With a few adjustments, this design can be used for non-transient event streams as well.\nAppendix [1] Many social networks provide APIs for gathering account level metrics so you don\u0026rsquo;t need to do this aggregation yourself. However, there are certain metrics which are not supported and cause us to have to do the aggregation ourselves.\n[2] While Redis sorted-sets do have the ability to atomically pop elements from the top or bottom of the sorted set, it unfortunately does not have an atomic way to query and remove elements from the middle. This means that we have to make two separate commands: ZRANGE to get the values and ZREM to remove the key after we publish the event. The consequence of this is that there is are race conditions if you have multiple threads looking for folded events to publish. This means a folded event can be published more than once. If this is a real issue you can consider using global locks or partitioning the key-space, with the trade-off of added complexity.\n[3] You should have some reasonable initial guess for your folding window. If not, take a sampling of events and determine it retrospectively offline. If your latency requirements allow for it, start with a larger folding window and gradually reduce it while monitoring your folding ratio.\n[4] Even though ElasticSearch has APIs for updating documents, under the hood it is actually indexing a new document and marking the old document for deletion.\n","ref":"/posts/event_folding/"},{"title":"Building a Comfortable Dev Environment","date":"","description":"","body":"Being a software developer can be overwhelming at times. There are an endlessly diverse set of tools, technologies, languages and frameworks to choose from. To make it worse, that list grows larger and larger each day. Tools like git, docker, docker-compose, kubernetes, ssh, curl, sed, awk, grep, jq etc. are all tools we use multiple times a day.\nMost of these tools have a small subset of commands / flags or use-cases that we use most often. In more complex cases, these tools can be piped together. As the years go by, with repetition, you end up memorizing these commands. This is great for productivity, but it takes time!\nIn the meantime, while you have yet to internalize these commands, your brain needs to stop (even for a second) to think: \u0026ldquo;What was that flag again?\u0026rdquo;\nThese small but frequent context switches cause us to lose a step on what is more important at the time: solving that bug, or finishing that feature. The solution is simple. Keep a cheatsheet of all your favorite commands written on cue cards. Tuck that cue card under your pillow and each night before bed, recite all the commands in their entirety out loud as you eventually fall asleep.\n\u0026hellip;\nNo. We\u0026rsquo;re not in school anymore (well some of you may be). There is a (much) better way of removing this unnecessary cognitive load so you can always stay focused on your task at hand.\nIn this article I am going to talk about a few things I do to speed up my development flow. I\u0026rsquo;ve found this incredibly valuable and I hope you do too. Best of all, you can take as much or as little from this flow as suites you.\nCustomize it, change it, improve it and share your results, I would love to hear it.\nLet\u0026rsquo;s get started with the simplest thing you can do:\nThe Shell Aliases A shell alias is a shortcut that you can define for any command. They are simple to create and can save you a lot of time and memorizing. For example, instead of typing ls -lah each time, you can create an alias la that would do the exact same thing:\nalias la=\u0026#34;ls -lAh\u0026#34; Aliases act as a run-time replacement of your alias with the defined command. This means you can augment your alias on the fly:\nla -R would translate to:\nls -lAh -R Aliases are a great way to turn that awkward to remember or type command into something dead simple that you can remember immediately. There is no alias too silly or simple. The following are probably my most commonly used aliases:\nalias cd.=\u0026#34;cd ..\u0026#34; alias cd..=\u0026#34;cd ../..\u0026#34; alias cd...=\u0026#34;cd ../../..\u0026#34; Lastly, aliases need to be loaded into your shell. The most common way of loading aliases is to define them in your .bashrc so that they are defined whenever you open a new shell.\nShell Functions An even more flexible option to aliases are shell functions. These act similarly to an alias but instead of simply replacing text being sent to your shell\u0026rsquo;s REPL, it executes the shell code you\u0026rsquo;ve defined. Use shell functions whenever you want to do something more complex than just shortcuts such as provide input arguments, flags and pipe input. Below is an example function to extract a specific JSON key-value from input:\n# Reads from JSON from stdin and print outs the value of the specified key # $1: Key to print # Example: echo \u0026#39;{\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2}\u0026#39; | jsonparsekey b # \u0026gt; 2 function jsonparsekey() { local KEY=$1 python -c \u0026#34;import sys,json; s=json.loads(\u0026#39;\u0026#39;.join([l for l in sys.stdin])); print s[\u0026#39;$KEY\u0026#39;]\u0026#34; } I find this useful for filtering response data of HTTP APIs as I am debugging.\nCheck out some of my other aliases and shell functions.\nUpgrade from Bash Bash is ubiquitous and great, but over the years people have developed much more full-featured and customizable shells such as zsh and fish. I would highly recommend looking into these shells or the many others that exist. I personally use zsh with the oh-my-zsh configuration manager to extend and customize the shell to my liking.\nStoring your Configuration Over the years you will likely be using many different computers. I personally have my home desktop, a personal laptop and my work laptop. To keep configurations on all my devices in sync I have a git repository which stores all my aliases, shell functions, zsh plugins and common system packages. This repo also prevents me from losing and having to recreate configurations as I reinstall my OS or format my hard drives.\nI even have a script that loads the repository, keeping my .zshrc file edits minimal:\n.zshrc\nsource ~/lobocv/mysetup/load_aliases.sh Containerized Environments Docker and docker-compose make it simple to create a customized and reproducible development environment for your team\u0026rsquo;s software projects. In addition to the docker container, I have several scripts which help make common tasks dead simple. These tasks could be as trivial as building the docker image(s), starting and entering the development container and tearing it down when I\u0026rsquo;m finished.\nThe Docker Container At the heart of the development environment is the development container. This container contains all the tools and dependencies I would need in order to perform my daily tasks. If a public docker image with the main dependencies installed already exists, I use that as the base image for the dev container. For example, when I write Go applications, I use the golang docker image which already includes the go toolchain. Afterwards, I install any additional dependencies I need for the project such as protocol buffers and linters.\nDockerfile\nFROM golang:1.16 RUN go get google.golang.org/protobuf/cmd/protoc-gen-go \\ google.golang.org/grpc/cmd/protoc-gen-go-grpc Docker-compose makes it simple to orchestrate several containers which can then communicate with one another via a common network or share volumes with the host or one another. In the following docker-compose example config, I define the development container, dev, and mount my local filesystem (./) to a /src folder inside the container. This allows my development container have access to code in the repository and immediately see changes being made to them from my IDE.\nThe example configuration below also starts up a MongoDB container named mongo.\ndocker-compose.yaml\nversion: \u0026#34;3\u0026#34; services: dev: build: context: . dockerfile: ./Dockerfile command: sleep 1000000 volumes: - ./:/src mongo: image: mongo:4.4-bionic Accessing the Development Environment To make it fast and simple to get going, I write a bash script that starts the container(s) and enters the development container\u0026rsquo;s shell, which I call the devshell:\ndevshell.sh\n#!/usr/bin/env bash PROJECT_NAME=myproject DEV_CONTAINER=dev RC_FILE=/src/.devshell_bashrc # If the -b flag is provided then build the containers before entering the shell if [ \u0026#34;$1\u0026#34; == \u0026#34;-b\u0026#34; ]; then docker-compose -p ${PROJECT_NAME} build fi # Check if the dev container is already up. If it\u0026#39;s not, then start it [ ! \u0026#34;$(docker ps | grep ${DEV_CONTAINER})\u0026#34; ] \u0026amp;\u0026amp; docker-compose -p ${PROJECT_NAME} up -d ${DEV_CONTAINER} # Enter the dev shell and load the rc file docker exec -it -e \u0026#34;TERM=xterm-256color\u0026#34; \u0026#34;${PROJECT_NAME}_${DEV_CONTAINER}_1\u0026#34; bash --rcfile ${RC_FILE} This script checks if the docker container is already running so that it does not always need to call the (somewhat) slow docker-compose up command.\nCustomizing the environment You may have noticed that in devshell.sh I provided an --rcfile parameter to bash. This allows us to setup any customized environment we want in the devshell. This can contain functions or aliases that are specific to your project. I like to have this script define a help() function and print it upon entry of the shell. This helps new team members joining the project get accustom to what features exist in the devshell. It also helps broadcast any improvements added to the shell and acts as reference documentation for the devshell.\n.devshell_bashrc\n# Define any common useful aliases or functions for the team alias gotest=\u0026#34;go test ./...\u0026#34; alias testify=\u0026#34;go test -testify.m\u0026#34; alias lint=\u0026#34;golangci-lint run -v ./...\u0026#34; # Set default cd to go to project root alias cd=\u0026#39;HOME=/src cd\u0026#39; function help() { echo \u0026#34; ======================================= Welcome to the dev shell ======================================= Type \u0026#34;help\u0026#34; in the shell to repeat this message. Additional shell customizations can be loaded by creating a .customrc file in the root of the project. Here is a list of common commands you can do: * gotest: Run all go tests in the current directory * lint: Run golangci-lint in the current directory * testify: Run a testify test by name Arguments: 1 : Regex to match test names \u0026#34; } # Source any user-specific / personal aliases or functions if [[ -f .customrc ]]; then echo \u0026#34;Custom shell configuration found. Loading...\u0026#34; source .customrc fi help Personalizing the Shell At the end of the .devshell_bashrc script above, we source a .customrc file (if it exists). Each member of your team can use this file to personalize their devshell. Be sure to add .customrc to your project\u0026rsquo;s .gitignore so that someone does not accidentally share their own personal scripts.\nHere is an example of some additional personalization I do to my devshell:\n.customrc\n#!/bin/bash echo \u0026#34;Hi Calvin!\u0026#34; alias cd.=\u0026#34;cd ..\u0026#34; alias cd..=\u0026#34;cd ../..\u0026#34; alias cd...=\u0026#34;cd ../../..\u0026#34; alias la=\u0026#34;ls -lah\u0026#34; alias gofmt=\u0026#34;gofmt -w -s .\u0026#34; alias run=\u0026#39;go run ./...\u0026#39; While these examples are pretty general purpose, there are many ways you can tailor your environment to speed up your development flows. Be creative! Here are some ideas:\nChanging to a particular directory which I often use Run a particular tool such as generating proto files Building and running a program / service via a regex Changing to a particular project directory via a regex Shortcuts interacting with ElasticSearch: List/delete aliases, templates, indices Teardown and Cleanup Tearing down the containers is as simple as calling docker-compose down. Although this is a simple command, having it defined as a script opens the door to add more functionality such as only shutting down certain containers. It also provides nice symmetry to devshell.sh and makes things dead simple.\ndevdown.sh\n#!/usr/bin/env bash PROJECT_NAME=myproject docker-compose -p ${PROJECT_NAME} down --remove-orphans Try it yourself A working example of this development environment can be found at my project-bootstrap repository. Feel free to try it out!\nMake it your own There is a lot of focus on optimizing your code or algorithms, but optimizing your development efficiency is one of the highest impact changes you can make. These are just some examples of what you can do. Be creative and come up with your own optimizations. Keep track of the context changes and inefficiencies in your workflow and you\u0026rsquo;ll be surprised how much of an impact it can have on your output. Keep in mind, that unlike code, these gains with follow you throughout your professional career.\n","ref":"/posts/dev_environment/"},{"title":"Recursively Merging JSONB in PostgreSQL","date":"","description":"","body":" In addition to storing primitive data types such as INT, FLOAT and VARCHAR, PostgreSQL supports storing JSON and binary JSON (JSONB). These JSON types have a wide variety of functions and operators[1]. One of the more common and useful operators is the concatenation operator, ||, which concatenates two jsonb values into a new JSONB value.\nExample:\npostgres=\u0026gt; SELECT \u0026#39;{\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2}\u0026#39;::jsonb || \u0026#39;{\u0026#34;b\u0026#34;: 5, \u0026#34;c\u0026#34;: 6}\u0026#39;::jsonb as result; result -------------------------- {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 5, \u0026#34;c\u0026#34;: 6} However, this concatenation is limiting. For one, if a key is present in both arguments, the second value will completely overwrite the first. This is a problem for nested objects. The following example attempts to update the \u0026quot;author.age\u0026quot; value from 30 to 31, but also ends up removing the author.name field.\nSELECT \u0026#39;{\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 30, \u0026#34;name\u0026#34;: \u0026#34;Calvin\u0026#34;}}\u0026#39;::jsonb || \u0026#39;{\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 31}}\u0026#39;::jsonb as result; result -------------------- {\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 31}} In order to preserve objects and have their fields merged instead of overwritten, we need to write a custom function.\nHere is the full function which recursively merges two JSON objects A and B:\nCREATE OR REPLACE FUNCTION jsonb_recursive_merge(A jsonb, B jsonb) RETURNS jsonb LANGUAGE SQL AS $$ SELECT jsonb_object_agg( coalesce(ka, kb), CASE WHEN va isnull THEN vb WHEN vb isnull THEN va WHEN jsonb_typeof(va) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; OR jsonb_typeof(vb) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; THEN vb ELSE jsonb_recursive_merge(va, vb) END ) FROM jsonb_each(A) temptable1(ka, va) FULL JOIN jsonb_each(B) temptable2(kb, vb) ON ka = kb $$; This function may be a bit hard to digest, so let\u0026rsquo;s break it down:\nSELECT jsonb_object_agg( ... ) FROM jsonb_each(A) temptableA(ka, va) FULL JOIN jsonb_each(B) temptableB(kb, vb) ON ka = kb jsonb_object_agg is a built-in postgresql function which aggregates a list of (key, value) pairs into a JSON object. This is what creates the final merged JSON result. Here we are applying jsonb_object_agg on the results of an in-memory temporary table that we are creating on the fly.\nTemporary tables jsonb_each() is a built-in postgresql function that iterates a JSON object returning (key, value) pairs. We call this function on both input JSON object A and B and then store the results in temporary tables temptableA and temptableB respectively.\ntemptableA(ka, va) is the definition of a temporary table with columns ka and va for the key and value results of jsonb_each(). This is where ka and va are first introduced. We do the exact same thing for JSON object B to get kb and vb.\nNext we do a FULL JOIN with the two temporary tables on the key column. This gives us one table that has all the (key, value) pairs from both JSON objects A and B. Below is an example of what the results of that table may look like:\nka va kb vb likes 5 likes 10 comments 3 shares 1 impressions 65 impressions 130 Table 1: An example of a FULL JOIN with two temporary tables produced by jsonb_each()\nIt is this table from which we select the input to jsonb_object_agg(). As we iterate through the rows of this joined temporary table, we need to determine which key (ka or kb) and value (va or vb) we want to place in the resultant JSON object.\nSelecting the Key coalesce(ka, kb) coalesce is a built in postgresql function that returns the first non null value it is given. In this case it will choose ka if kb is null or kb if ka is null. Since we performed our FULL JOIN on columns ka = kb, we are guaranteed to have a non-null value for either ka or kb. When both ka and kb are non-null, they will be the same value.\nSelecting the Value CASE WHEN va isnull THEN vb WHEN vb isnull THEN va WHEN jsonb_typeof(va) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; OR jsonb_typeof(vb) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; THEN vb ELSE jsonb_recursive_merge(va, vb) END To select the value, we have a switch statement. The first two cases chooses the non-null value when one of the values is null. The third case is when both va and vb are defined and not both JSON objects themselves. In this case we choose vb over va (remember we are merging B into A). The final case (else) handles the situation where va and vb are both JSON objects. In that situation we recursively call the jsonb_recursive_merge on va and vb.\nUsing the function One common use for this function is to upsert a row. In an upsert, when the row exists, you want to update it and when it doesn\u0026rsquo;t, you want to insert a new one. To do this, you would use an INSERT statement with the ON CONFLICT (col1,..., colN) DO UPDATE SET clause. The columns in the clause specify the columns of a unique index. Following the clause is a list of column_name = \u0026lt;expression\u0026gt; statements that decide just how each column is to be updated.\nBelow is an example of updating a table of tweet metrics:\nINSERT INTO tweets (id, metrics) VALUES (1, \u0026#39;{\u0026#34;likes\u0026#34;: 22, \u0026#34;comments\u0026#34;: 12}\u0026#39;) ON CONFLICT (id) DO UPDATE SET metrics = jsonb_recursive_merge(tweets.metrics, excluded.metrics); In the statement above, if a row with the same ID exists, it will call the jsonb_recursive_merge function on the current value, tweets.metrics, and the inserted value, excluded.metrics (the excluded table is the name of the special table representing rows proposed for insertion[2]).\nLimitations When we designed our jsonb_recursive_merge function we had to decide what \u0026ldquo;merge\u0026rdquo; meant to us. We decided that an overwrite of a value constitutes a \u0026ldquo;merge\u0026rdquo;. But what about values that are arrays? One could argue that merging two arrays [1, 2, 3] and [4, 5, 6] should result in [1,2,3,4,5,6]. It really all depends on the context of what you are trying to do.\nIf you want to also merge the values of arrays you can add an extra case statement that appends the values when both va and vb are arrays:\nWHEN jsonb_typeof(va) = \u0026#39;array\u0026#39; AND jsonb_typeof(vb) = \u0026#39;array\u0026#39; THEN va || vb However, be aware that this will apply to all arrays encountered in the JSON objects.\nAnd there you have it, a custom PostgreSQL function that merges two JSON objects, preserving and merging any nested objects. I\u0026rsquo;d like to thank and give credit to klin and his very helpful StackOverflow answer which brought me to a solution to this problem.\nFurther Reading [1] JSON Functions and Operators\n[2] PostgresSQL Insert Documentation\n","ref":"/posts/recursive_jsonb_merge/"},{"title":"About","date":"","description":"","body":"I am a backend software developer that has a passion for software architecture, scalability, development efficiency and clean code. I love learning and sharing what I have learned with others.\nAside from work, I love to play electric guitar, renovate my home and be out in nature.\n","ref":"/about/"}]