[{"title":"Defusing an ElasticSearch Mapping Explosion with Slots","date":"","description":"","body":"ElasticSearch (and similarly OpenSearch[1]) is a popular OLAP database that allows you to quickly search and aggregate your data in a rich and powerful way. It is a mature storage technology build on top of Apache Lucene that has been used to back many online storefronts and analytical processing products around the world. Under the hood, ElasticSearch uses Lucene to index each field in the document so that queries can be executed efficiently.\nIn order to provide rich searching capabilities, ElasticSearch creates indices for each field it receives. The type of index created is determined by the data type of the field. Possible data types include numeric types such as integer, float and double as well as two string data types, keyword and text. The list of all indexed fields (and how they are indexed) are stored in what is called an index mapping. By default, ElasticSearch will create a mapping entry when a new field is encountered by deducing it\u0026rsquo;s data type[2]. This is called dynamic mapping.\nThe problem with dynamic mapping is that once ElasticSearch introduces a field to the mapping, it can never be removed. The index mapping also consumes cluster memory and must be replicated to all nodes. If you are not careful, leaving dynamic mapping enabled can lead to very large index mappings and slow down your cluster. This is called a mapping explosion. For this reason, it is recommended to disable dynamic mapping and enable it sparingly only for objects that you can trust have a finite number of fields[3].\nIn many cases this is not an issue and it is not restrictive to disable dynamic mapping. However, in some cases a user may determine the name of a field. In such situations, one must come up with a strategy to mitigate the risk of an eventual mapping explosion. The rest of this article talks about how we mitigated a mapping explosion in HootSuite\u0026rsquo;s analytics products.\nThe problem context At Hootsuite, we use ElasticSearch to store social media content and their associated metrics such as likes, impressions shares and comments. With ElasticSearch, customers can search for content across all their social networks in a multitude of ways. They can also aggregate metric values from their content over a time range to give them a view of their social marketing performance. These metrics are retrieved from the social network APIs and stored in separate fields in ElasticSearch under a metrics object. These fields have the same name as the metric and are common among all users of the social network. Since there are a finite number of metrics, there is no risk of a mapping explosion. However, we also collect metrics from offsite attributions such as Google and Adobe Analytics. On these platforms, users can name and define their own metrics. These are the metrics we need to be careful about. Each customer now has the ability to permanently increase our mapping size by creating a unique metric name. The problem is even more serious when you consider that these mapping entries will be around long after a customer has stopped using that metric.\nDisabling these fields in the mapping is not an option. We needed to devise a way to still provide the search and aggregation functionality that is so critical to our product functionality, yet also have a stable long tem solution. To overcome this, we came up with a system we called Slots.\nDefusing a mapping explosion With the slots approach, we gain control of unbounded mapping growth by predefining a fixed number of fields (the slots) and translating them in the application code to the user-defined values. We call the application-level translation from metric to slot the slot-mapping.\nEach user will have their own slot-mapping, meaning that the metric referred to by each slot will be different for each user. The number of slots we need to allocate needs to be, at a minimum, equal to the amount that the customer with the highest number of custom-named metrics has[4]. From looking at our own data, the customer with the largest amount of custom metrics had about 200 metrics. To be safe, we allocated 1000 slots.\nTrade offs The slots approach is not without it own caveats. For one, it makes debugging and reading data more difficult. Now if we inspect our data with third party tools such as Kibana, rather than seeing a field named custom_metric_c, we see slot_3. We need to refer back to the slot-mapping to determine which slot refers to the metric we are looking for.\nSecond, we must be careful that there are no requests for data that combine documents that use different slot-mappings. You cannot aggregate slot_2 across data from two different customers because the metric slot_2 refers to will be different. Luckily, aggregating across customers is not a valid business use-case and in fact goes again our multi-tenant principles.\nThird, a customer may eventually hit their 1000 slot limit by renaming or adding new metrics that they use temporarily. This has yet to happen but there are a few avenues for when it possibly does. We can reclaim some of the slots that are no longer in use by going through data and removing the values from documents. Alternatively, we can simply increase the maximum slots.\nDefining the slot mapping Each metric, whether it be from the social network or user defined, has a unique ID and some metadata associated with it called the Metric Definition. At first it was tempting to store the slot number on the metric definition itself, however we decided against it because slots are an implementation detail of the fact that we use ElasticSearch as a storage backend. By exposing the slots outside of the service performing the translation to/from metric name to slot, we are weakening our abstraction.\nInstead, we had the services that write to ElasticSearch each define their own slot-mappings. The slot mapping is defined the first time the service writes the metric to ElasticSearch. The flow requires us to keep a counter for the current number of slots for a user and also the relationship between slot and metric name.\nTable: slot_counters\n   user_id n_slots     1 2   2 4   3 0    Table 1: The slot_counters table keeps track of the number of slots each user has used.\nTable: slot_mappings\n   user_id slot metric     1 1 visits   1 2 bounces   2 1 website_visits   2 2 website_pageviews   2 3 ecommerce_revenue   2 4 goal_values    Table 2: The slot_mappings table contains the relationships between slot and metric for each user. It has a unique constraint on combinations of (user_id, slot).\nWhen a new metric is written, the counter is incremented and a new slot_mapping row is written. The slot_mapping table must also have a unique compound index on the user_id and slot columns. This unique index prevents potential race conditions from multiple API requests trying to grab the next slot number at the same time. In the case of a race condition, one of the attempts to write to the slot_mapping table will fail on the unique-constraint and we can roll back the transaction and simply retry again.\nBefore writing to ElasticSearch, all metric names are converted to their slot numbers. When reading or aggregating metrics, the slot names retrieved from ElasticSearch are translated back to the original metric names with the same slot_mapping table. This whole process is completely transparent to the user of the API.\nA small LRU caching layer is added around the metric slot-mapping values to reduce the number of times we need to go to the database. Since these values are static and small, this ends up working very well.\nConclusion ElasticSearch / OpenSearch works well for indexing complex data schemas but has limitations when the the number of fields is unbounded. Using application-level translation with the Slots approach, we can multiplex a fixed number of fields to represent a larger number of fields and prevent a mapping explosion. The trade off is an overall increase in complexity and reduction to the ease of debugging. The performance impact of adding an additional translation layer is negligible with the help of an LRU cache.\nFootnotes [1] In 2021, Elastic.co, the company that owns the license to ElasticSearch, changed their license to be more restrictive and in conflict of open source values. In response, Amazon created a fork of ElasticSearch 7.10.2 named OpenSearch which continues the Apache 2.0 license.\n[2] ElasticSearch prevents you from shooting yourself in the foot too much by setting an upper limit on the number of fields in a mapping. As of writing this, the default is 1000. This limit can be increased at any time.\n[3] Dynamic mapping has another problem, it will set the field settings to general purpose presets. If you want more control of your fields, such as defining a custom analyzer, you should define the field mappings explicitly.\n[4] Generally speaking, having hundreds or even thousands of fields in an ElasticSearch mapping is not as much of an issue as the unbounded growth of the mapping.\n","ref":"/posts/elasticsearch_slots/"},{"title":"Reducing System Load With Event Folding","date":"","description":"","body":"code.has-jax {font: inherit; font-size: 100%; background: inherit; border: inherit;}  MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\\\(','\\\\)']], skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'] // removed 'code' entry } }); MathJax.Hub.Queue(function() { var all = MathJax.Hub.getAllJax(), i; for(i = 0; i   One of the prevailing forms of communication in modern microservice architectures is asynchronous messaging. This is the backbone of the event-driven architecture model. In this model, services send messages to a message broker, which then distributes (publishes) the messages to interested (subscribed) clients. A message can be used to describe an event in the system, such as the creation or update of an entity. This allows you to loosely couple different components of your system by having them publish or subscribe to events that they care about.\nBut what happens if there is a subscriber that wants to react to a group of events as an aggregate? In other words, what if you wanted to do something only after all the update events stop being published, rather than reacting to each and every one as they come in?\nLet\u0026rsquo;s look at a concrete example.\nAggregating Social Media Metrics Let\u0026rsquo;s say you are building a system that collects metrics on your social media posts. Your system needs to provide both an individual view of post performance and view of posts' performance as a whole. Let\u0026rsquo;s call metrics on individual posts post-level metrics and metrics on your posts as a whole account-level metrics. To keep things simple, let\u0026rsquo;s assume all post-level metrics can be aggregated using a sum:\n$M_{account} = \\sum_{i=0}^{N_{posts}} M_{post_{i}}$\nFrom the equation above, it is clear that account-level metrics depend on the post-level metrics. Every time a metric on a post changes, we will need to recompute this sum[1]. In an event-driven architecture, you may have a service that performs this aggregation by listening to post-metric-updated events. If many of these events are produced within a short period of time, the service will be computing this sum many times over, with only the last computation being valuable (all other computations would produce a stale value). What we would really like to do in this situation is detect when the last post-metric-updated event comes in and only then compute the sum. But how do we know which event is the last?\nDetermining which event is last is actually a difficult problem and can\u0026rsquo;t be solved in a general context. The concept of \u0026ldquo;last\u0026rdquo; is application specific. In our example, a post can have it\u0026rsquo;s metrics updated at any time. If we are okay with some degree of staleness to our account metrics, we can implement a delayed computation that occurs only after we stop receiving events for a certain amount of time.\nThe Ideal Event Ideally, what we would like to have is to have a slightly different event than our post-metric-updated event. What we really want is a post-metric-last-updated-X-minutes-ago event, where X is some acceptable delay. If each post-metric-updated event contains a list of all the updated metrics, then the post-metric-last-updated-X-minutes-ago event should contain the superset of all the metrics that were updated. Let\u0026rsquo;s define the time duration, X, the folding window because it is the minimum duration of time at which similar events will be folded into a single event.\nBut how do we produce such an event?\nWe will need some code that will do the following for each account:\n Listen for post-metric-updated events Keep a super-set of all the updated metrics in the events Keep track of the last time a post-metric-updated event comes in Produce a post-metric-last-updated-X-minutes-ago when we don\u0026rsquo;t receive an event for X minutes  This turns out to be fairly simple to implement with the use of Redis sorted-sets.\nSorted Set A sorted set is a unique collection of strings that have an associated score. The score is used to sort the entries within the set. For example, a sorted set would be a perfect data structure for storing a user scoreboard:\n   Rank Name Highscore     1 Sharon 1050   2 Abdul 800   3 Calvin 780   4 Pirakalan 660   5 Nirav 600    We can insert the data above into the sorted set in any order and it will maintain the entries in the order of high-score.\n\u0026gt;\u0026gt; ZADD highscores 780 Calvin \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 1050 Sharon \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 660 Pirakalan \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 600 Nirav \u0026gt;\u0026gt; (integer) 1 \u0026gt;\u0026gt; ZADD highscores 800 Abdul \u0026gt;\u0026gt; (integer) 1 It is then very simple to ask the sorted set for the top N results with ZREVRANGE. (You could also get the results sorted by ascending order by using ZRANGE)\n\u0026gt;\u0026gt; ZREVRANGE highscores 0 5 WITHSCORES 1) \u0026#34;Sharon\u0026#34; 2) 1050 3) \u0026#34;Abdul\u0026#34; 4) 800 5) \u0026#34;Calvin\u0026#34; 6) 780 7) \u0026#34;Pirakalan\u0026#34; 8) 660 9) \u0026#34;Nirav\u0026#34; 10) 600 Or get values within a range of scores with ZRANGEBYSCORE.\n\u0026gt;\u0026gt; ZRANGEBYSCORE highscores 700 800 WITHSCORES 1) \u0026#34;Abdul\u0026#34; 2) 800 3) \u0026#34;Calvin\u0026#34; 4) 780 And of course you can delete keys from the sorted-set with ZREM.\n\u0026gt;\u0026gt; ZREM highscores Abdul (integer) 1 Folding Events Since the sorted set can only have unique values, we can use it to \u0026ldquo;fold\u0026rdquo; a group of events into a single event. In order to do that, we need to remove unique information from the events such that they serialize to the same string value. We can use the time of the event as the score in the sorted-set. Each time an event is added, we update the score, thus keeping track of the last event in the group. Our sorted-set then holds a time-ordered collection of events, where each element represents a folded view of events from each group. We can then regularly query the sorted-set with ZRANGEBYSCORE to get values with timestamps (scores) before time.Now()-X. These values are then published as post-metric-last-updated-X-minutes-ago events and removed from the sorted set[2].\nBack to our example, the post-metric-updated event has the following schema:\n// PostMetricUpdatedEvent is published when metrics change on a post. It contains the changed metric values. type PostMetricUpdatedEvent struct { PostID string `json:\u0026quot;id\u0026quot;` AccountID string `json:\u0026quot;account_id\u0026quot;` UpdatedMetrics map[string]float64 `json:\u0026quot;metrics\u0026quot;` } And we have the following events being published, shown below in their JSON serialized form:\n{\u0026quot;id\u0026quot;: \u0026quot;post_1\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;likes\u0026quot;: 10, \u0026quot;shares\u0026quot;: 5}} {\u0026quot;id\u0026quot;: \u0026quot;post_2\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;comments\u0026quot;: 25, \u0026quot;impressions\u0026quot;: 16}} {\u0026quot;id\u0026quot;: \u0026quot;post_3\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;likes\u0026quot;: 5, \u0026quot;shares\u0026quot;: 2}} {\u0026quot;id\u0026quot;: \u0026quot;post_4\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;comments\u0026quot;: 33, \u0026quot;impressions\u0026quot;: 8}} {\u0026quot;id\u0026quot;: \u0026quot;post_5\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_2\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;likes\u0026quot;: 12, \u0026quot;shares\u0026quot;: 15}} {\u0026quot;id\u0026quot;: \u0026quot;post_6\u0026quot;, \u0026quot;account_id\u0026quot;: \u0026quot;account_2\u0026quot;, \u0026quot;metrics\u0026quot;: {\u0026quot;likes\u0026quot;: 3, \u0026quot;shares\u0026quot;: 1}} These 6 events span two unique accounts (account_1 and account_2) and should therefore ideally create two separate post-metric-last-updated-X-minutes-ago events. Remember, a sorted set stores unique strings, so we cannot just insert these JSON strings into the sorted-set as is. If we did, they would be stored as 6 different entries in the set. We need to identify the group key for all the events that we want to fold. The group key should remove all uniquely identifying information in the event such that events within the same group have the same serialized value. In our example, the group key would be just the account_id field, since we just want one event per account:\n{\u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;} {\u0026quot;account_id\u0026quot;: \u0026quot;account_2\u0026quot;} But wait\u0026hellip; We just lost a bunch of information from our events! That\u0026rsquo;s right. I never said this folding was lossless! In some cases this may be acceptable, you may be able to gather that information elsewhere, say from an API call, or it may not be relevant for what you are trying to do on the subscriber side.\nIn cases where you do need that information, we can store it separately in a database as we are inserting values into the sorted-set. When querying the sorted-set for events to publish, we can \u0026ldquo;unfold\u0026rdquo; the event by enriching it with the information we stored in the database.\nIn our example, we need a list of metrics that have changed so that we know which metrics to aggregate. We can easily keep track of these metrics in a regular Redis set. When we query the sorted-set to get the folded event older than X, we also grab the metrics that have changed from the regular set. We can then assemble the post-metric-last-updated-X-minutes-ago event:\ntype PostMetricUpdatedEvent struct { AccountID string `json:\u0026quot;account_id\u0026quot;` UpdatedMetrics []string `json:\u0026quot;metrics\u0026quot;` } which in our example looks like:\n{\u0026quot;account_id\u0026quot;: \u0026quot;account_1\u0026quot;, \u0026quot;metrics\u0026quot;: [\u0026quot;likes\u0026quot;, \u0026quot;shares\u0026quot;, \u0026quot;comments\u0026quot;, \u0026quot;impressions\u0026quot;]} {\u0026quot;account_id\u0026quot;: \u0026quot;account_2\u0026quot;, \u0026quot;metrics\u0026quot;: [\u0026quot;likes\u0026quot;, \u0026quot;shares\u0026quot;]} The service performing the aggregation can then subscribe to these events and be notified only once metrics have stopped being updated. We can rest assured that we will compute the value at most once in the X minutes since the last time a post metric was updated.\nFolding Ratio To get a feel for how long we should set our folding window to be, we can calculate the folding ratio, ${F}_{R}$:\n${F}_{R}=\\frac{N_F}{N_E}$\nwhere $N_E$ = Number of fold-eligible events and $N_F$ = Number of actually folded events.\nWhen inserting into a Redis sorted-set with the ZADD operator, Redis will tell you how many new elements were added to the sorted-set (score updates excluded). Assuming we are inserting one event at a time, the number of folded events is measured by how many times the ZADD operation returns 0 and the number of new events is measured by how many times ZADD returns 1. With this in mind, we can measure the following values:\n$N_E= N_{Total} - Count(ZADD == 1)$\n$N_F$ = Count(ZADD == 0)\n$F_R = \\frac{Count(ZADD == 0)}{N_{Total} - Count(ZADD == 1)}$\nwhere $N_{Total}$ is the total number of events.\nBut there is a caveat; we are also popping elements off of the sorted set in order to publish the folded event. If the folding window is too short for the timing of our incoming events, we are going to be removing an element from the sorted set just before adding a new event that would have otherwise been folded. In this situation, we will be over measuring our number of new events and under measuring the number of folded events. Another way to look at this is that the value of ZADD only acts as a proxy for the folded count from Redis' perspective.\nBelow is an example of what that could look like:\n{\u0026#34;id\u0026#34;: \u0026#34;post_1\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 10, \u0026#34;shares\u0026#34;: 5}} -\u0026gt; ZADD Returns 1 (new) {\u0026#34;id\u0026#34;: \u0026#34;post_2\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 25, \u0026#34;shares\u0026#34;: 16}} -\u0026gt; ZADD Returns 0 (folded) {\u0026#34;id\u0026#34;: \u0026#34;post_3\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 5, \u0026#34;shares\u0026#34;: 2}} -\u0026gt; ZADD Returns 0 (folded) Publish folded event {\u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;} {\u0026#34;id\u0026#34;: \u0026#34;post_4\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_1\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 33, \u0026#34;shares\u0026#34;: 8}} -\u0026gt; ZADD Returns 1 (fake new) {\u0026#34;id\u0026#34;: \u0026#34;post_5\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 12, \u0026#34;shares\u0026#34;: 15}} -\u0026gt; ZADD Returns 0 (new) Publish folded event {\u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;} {\u0026#34;id\u0026#34;: \u0026#34;post_6\u0026#34;, \u0026#34;account_id\u0026#34;: \u0026#34;account_2\u0026#34;, \u0026#34;metrics\u0026#34;: {\u0026#34;likes\u0026#34;: 3, \u0026#34;shares\u0026#34;: 1}} -\u0026gt; ZADD Returns 1 (fake new) If your system is processing a large number of folded events ($N_{Total} \\gg Count(ZADD == 1)$) and your folding window is reasonable[3], then the error becomes negligible and the folding ratio simplifies further:\n${F}_{R}\\approx\\frac{Count(ZADD == 0)}{N_{Total}}$\nIf it is really important that you have the most optimal folding ratio, you will need to do some offline analysis such as looking at historical events within a time range. In this case, you will actually know the number of events that should be folded. In most cases, the timing of your events will be irregularly spaced and you will never consistently achieve 100% folding. Even then, any non-zero folding is still cutting the amount of work your system is doing.\nThe example we have been using throughout this article is a real world scenario. In production we see anywhere between 85% and 98% folding over hundreds of thousands of events per day. Without event folding, we would have overloaded our database a long time ago and be forced to drop our event-driven approach in favour of something more traditional such as scheduled cron jobs.\nPlacement of the Event Folder The event folder can be placed on either the publisher side or on the consumer side. Where you decide to place the event folder ultimately comes down to how many consumers want folded events. By folding on the publisher side, you can reduce complexity for consumers downstream. However, if only one consumer cares about folding events, it may be cleaner to place the event folder on the consumer side and not have to make any changes at the producer. If you place the event folder on the publisher side, there is nothing wrong with publishing both folded and unfolded events, just be sure to place them on separate topics to prevent confusion.\nAnother Use-case: Database Syncing Another use-case where the event folder is highly effective is during database syncing. In a CQRS pattern, we can separate database technologies used for writing and reading by syncing all writes to the read database. Some databases such as ElasticSearch perform poorly for document updates[4]. Using an event folder we can reduce the number of writes to ElasticSearch, significantly reducing the number of deleted documents that need to be garbage collected.\nA Powerful Tool The event folder can be a powerful tool in your system\u0026rsquo;s architecture. It can be the difference in an event driven system\u0026rsquo;s viability. The event folder I have outlined in this article is designed around transient spikes in events. If your events are not transient, you may find that your folded events may never be published due to a constantly resetting folding window. With a few adjustments, this design can be used for non-transient event streams as well.\nAppendix [1] Many social networks provide APIs for gathering account level metrics so you don\u0026rsquo;t need to do this aggregation yourself. However, there are certain metrics which are not supported and cause us to have to do the aggregation ourselves.\n[2] While Redis sorted-sets do have the ability to atomically pop elements from the top or bottom of the sorted set, it unfortunately does not have an atomic way to query and remove elements from the middle. This means that we have to make two separate commands: ZRANGE to get the values and ZREM to remove the key after we publish the event. The consequence of this is that there is are race conditions if you have multiple threads looking for folded events to publish. This means a folded event can be published more than once. If this is a real issue you can consider using global locks or partitioning the key-space, with the trade-off of added complexity.\n[3] You should have some reasonable initial guess for your folding window. If not, take a sampling of events and determine it retrospectively offline. If your latency requirements allow for it, start with a larger folding window and gradually reduce it while monitoring your folding ratio.\n[4] Even though ElasticSearch has APIs for updating documents, under the hood it is actually indexing a new document and marking the old document for deletion.\n","ref":"/posts/event_folding/"},{"title":"Building a Comfortable Dev Environment","date":"","description":"","body":"Being a software developer can be overwhelming at times. There are an endlessly diverse set of tools, technologies, languages and frameworks to choose from. To make it worse, that list grows larger and larger each day. Tools like git, docker, docker-compose, kubernetes, ssh, curl, sed, awk, grep, jq etc. are all tools we use multiple times a day.\nMost of these tools have a small subset of commands / flags or use-cases that we use most often. In more complex cases, these tools can be piped together. As the years go by, with repetition, you end up memorizing these commands. This is great for productivity, but it takes time!\nIn the meantime, while you have yet to internalize these commands, your brain needs to stop (even for a second) to think: \u0026ldquo;What was that flag again?\u0026rdquo;\nThese small but frequent context switches cause us to lose a step on what is more important at the time: solving that bug, or finishing that feature. The solution is simple. Keep a cheatsheet of all your favorite commands written on cue cards. Tuck that cue card under your pillow and each night before bed, recite all the commands in their entirety out loud as you eventually fall asleep.\n\u0026hellip;\nNo. We\u0026rsquo;re not in school anymore (well some of you may be). There is a (much) better way of removing this unnecessary cognitive load so you can always stay focused on your task at hand.\nIn this article I am going to talk about a few things I do to speed up my development flow. I\u0026rsquo;ve found this incredibly valuable and I hope you do too. Best of all, you can take as much or as little from this flow as suites you.\nCustomize it, change it, improve it and share your results, I would love to hear it.\nLet\u0026rsquo;s get started with the simplest thing you can do:\nThe Shell Aliases A shell alias is a shortcut that you can define for any command. They are simple to create and can save you a lot of time and memorizing. For example, instead of typing ls -lah each time, you can create an alias la that would do the exact same thing:\nalias la=\u0026quot;ls -lAh\u0026quot; Aliases act as a run-time replacement of your alias with the defined command. This means you can augment your alias on the fly:\nla -R would translate to:\nls -lAh -R Aliases are a great way to turn that awkward to remember or type command into something dead simple that you can remember immediately. There is no alias too silly or simple. The following are probably my most commonly used aliases:\nalias cd.=\u0026quot;cd ..\u0026quot; alias cd..=\u0026quot;cd ../..\u0026quot; alias cd...=\u0026quot;cd ../../..\u0026quot; Lastly, aliases need to be loaded into your shell. The most common way of loading aliases is to define them in your .bashrc so that they are defined whenever you open a new shell.\nShell Functions An even more flexible option to aliases are shell functions. These act similarly to an alias but instead of simply replacing text being sent to your shell\u0026rsquo;s REPL, it executes the shell code you\u0026rsquo;ve defined. Use shell functions whenever you want to do something more complex than just shortcuts such as provide input arguments, flags and pipe input. Below is an example function to extract a specific JSON key-value from input:\n# Reads from JSON from stdin and print outs the value of the specified key # $1: Key to print # Example: echo '{\u0026quot;a\u0026quot;: 1, \u0026quot;b\u0026quot;: 2}' | jsonparsekey b # \u0026gt; 2 function jsonparsekey() { local KEY=$1 python -c \u0026quot;import sys,json; s=json.loads(''.join([l for l in sys.stdin])); print s['$KEY']\u0026quot; } I find this useful for filtering response data of HTTP APIs as I am debugging.\nCheck out some of my other aliases and shell functions.\nUpgrade from Bash Bash is ubiquitous and great, but over the years people have developed much more full-featured and customizable shells such as zsh and fish. I would highly recommend looking into these shells or the many others that exist. I personally use zsh with the oh-my-zsh configuration manager to extend and customize the shell to my liking.\nStoring your Configuration Over the years you will likely be using many different computers. I personally have my home desktop, a personal laptop and my work laptop. To keep configurations on all my devices in sync I have a git repository which stores all my aliases, shell functions, zsh plugins and common system packages. This repo also prevents me from losing and having to recreate configurations as I reinstall my OS or format my hard drives.\nI even have a script that loads the repository, keeping my .zshrc file edits minimal:\n.zshrc\nsource ~/lobocv/mysetup/load_aliases.sh Containerized Environments Docker and docker-compose make it simple to create a customized and reproducible development environment for your team\u0026rsquo;s software projects. In addition to the docker container, I have several scripts which help make common tasks dead simple. These tasks could be as trivial as building the docker image(s), starting and entering the development container and tearing it down when I\u0026rsquo;m finished.\nThe Docker Container At the heart of the development environment is the development container. This container contains all the tools and dependencies I would need in order to perform my daily tasks. If a public docker image with the main dependencies installed already exists, I use that as the base image for the dev container. For example, when I write Go applications, I use the golang docker image which already includes the go toolchain. Afterwards, I install any additional dependencies I need for the project such as protocol buffers and linters.\nDockerfile\nFROM golang:1.16 RUN go get google.golang.org/protobuf/cmd/protoc-gen-go \\  google.golang.org/grpc/cmd/protoc-gen-go-grpc Docker-compose makes it simple to orchestrate several containers which can then communicate with one another via a common network or share volumes with the host or one another. In the following docker-compose example config, I define the development container, dev, and mount my local filesystem (./) to a /src folder inside the container. This allows my development container have access to code in the repository and immediately see changes being made to them from my IDE.\nThe example configuration below also starts up a MongoDB container named mongo.\ndocker-compose.yaml\nversion: \u0026quot;3\u0026quot; services: dev: build: context: . dockerfile: ./Dockerfile command: sleep 1000000 volumes: - ./:/src mongo: image: mongo:4.4-bionic Accessing the Development Environment To make it fast and simple to get going, I write a bash script that starts the container(s) and enters the development container\u0026rsquo;s shell, which I call the devshell:\ndevshell.sh\n#!/usr/bin/env bash  PROJECT_NAME=myproject DEV_CONTAINER=dev RC_FILE=/src/.devshell_bashrc # If the -b flag is provided then build the containers before entering the shell if [ \u0026#34;$1\u0026#34; == \u0026#34;-b\u0026#34; ]; then docker-compose -p ${PROJECT_NAME} build fi # Check if the dev container is already up. If it\u0026#39;s not, then start it [ ! \u0026#34;$(docker ps | grep ${DEV_CONTAINER})\u0026#34; ] \u0026amp;\u0026amp; docker-compose -p ${PROJECT_NAME} up -d ${DEV_CONTAINER} # Enter the dev shell and load the rc file docker exec -it -e \u0026#34;TERM=xterm-256color\u0026#34; \u0026#34;${PROJECT_NAME}_${DEV_CONTAINER}_1\u0026#34; bash --rcfile ${RC_FILE} This script checks if the docker container is already running so that it does not always need to call the (somewhat) slow docker-compose up command.\nCustomizing the environment You may have noticed that in devshell.sh I provided an --rcfile parameter to bash. This allows us to setup any customized environment we want in the devshell. This can contain functions or aliases that are specific to your project. I like to have this script define a help() function and print it upon entry of the shell. This helps new team members joining the project get accustom to what features exist in the devshell. It also helps broadcast any improvements added to the shell and acts as reference documentation for the devshell.\n.devshell_bashrc\n# Define any common useful aliases or functions for the team  alias gotest=\u0026#34;go test ./...\u0026#34; alias testify=\u0026#34;go test -testify.m\u0026#34; alias lint=\u0026#34;golangci-lint run -v ./...\u0026#34; # Set default cd to go to project root alias cd=\u0026#39;HOME=/src cd\u0026#39; function help() { echo \u0026#34; ======================================= Welcome to the dev shell ======================================= Type \u0026#34;help\u0026#34; in the shell to repeat this message. Additional shell customizations can be loaded by creating a .customrc file in the root of the project. Here is a list of common commands you can do: * gotest: Run all go tests in the current directory * lint: Run golangci-lint in the current directory * testify: Run a testify test by name Arguments: 1 : Regex to match test names \u0026#34; } # Source any user-specific / personal aliases or functions if [[ -f .customrc ]]; then echo \u0026#34;Custom shell configuration found. Loading...\u0026#34; source .customrc fi help Personalizing the Shell At the end of the .devshell_bashrc script above, we source a .customrc file (if it exists). Each member of your team can use this file to personalize their devshell. Be sure to add .customrc to your project\u0026rsquo;s .gitignore so that someone does not accidentally share their own personal scripts.\nHere is an example of some additional personalization I do to my devshell:\n.customrc\n#!/bin/bash echo \u0026#34;Hi Calvin!\u0026#34; alias cd.=\u0026#34;cd ..\u0026#34; alias cd..=\u0026#34;cd ../..\u0026#34; alias cd...=\u0026#34;cd ../../..\u0026#34; alias la=\u0026#34;ls -lah\u0026#34; alias gofmt=\u0026#34;gofmt -w -s .\u0026#34; alias run=\u0026#39;go run ./...\u0026#39; While these examples are pretty general purpose, there are many ways you can tailor your environment to speed up your development flows. Be creative! Here are some ideas:\n Changing to a particular directory which I often use Run a particular tool such as generating proto files Building and running a program / service via a regex Changing to a particular project directory via a regex Shortcuts interacting with ElasticSearch: List/delete aliases, templates, indices  Teardown and Cleanup Tearing down the containers is as simple as calling docker-compose down. Although this is a simple command, having it defined as a script opens the door to add more functionality such as only shutting down certain containers. It also provides nice symmetry to devshell.sh and makes things dead simple.\ndevdown.sh\n#!/usr/bin/env bash  PROJECT_NAME=myproject docker-compose -p ${PROJECT_NAME} down --remove-orphans Try it yourself A working example of this development environment can be found at my project-bootstrap repository. Feel free to try it out!\nMake it your own There is a lot of focus on optimizing your code or algorithms, but optimizing your development efficiency is one of the highest impact changes you can make. These are just some examples of what you can do. Be creative and come up with your own optimizations. Keep track of the context changes and inefficiencies in your workflow and you\u0026rsquo;ll be surprised how much of an impact it can have on your output. Keep in mind, that unlike code, these gains with follow you throughout your professional career.\n","ref":"/posts/dev_environment/"},{"title":"Recursively Merging JSONB in PostgreSQL","date":"","description":"","body":" In addition to storing primitive data types such as INT, FLOAT and VARCHAR, PostgreSQL supports storing JSON and binary JSON (JSONB). These JSON types have a wide variety of functions and operators[1]. One of the more common and useful operators is the concatenation operator, ||, which concatenates two jsonb values into a new JSONB value.\nExample:\npostgres=\u0026gt; SELECT \u0026#39;{\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 2}\u0026#39;::jsonb || \u0026#39;{\u0026#34;b\u0026#34;: 5, \u0026#34;c\u0026#34;: 6}\u0026#39;::jsonb as result; result -------------------------- {\u0026#34;a\u0026#34;: 1, \u0026#34;b\u0026#34;: 5, \u0026#34;c\u0026#34;: 6} However, this concatenation is limiting. For one, if a key is present in both arguments, the second value will completely overwrite the first. This is a problem for nested objects. The following example attempts to update the \u0026quot;author.age\u0026quot; value from 30 to 31, but also ends up removing the author.name field.\nSELECT \u0026#39;{\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 30, \u0026#34;name\u0026#34;: \u0026#34;Calvin\u0026#34;}}\u0026#39;::jsonb || \u0026#39;{\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 31}}\u0026#39;::jsonb as result; result -------------------- {\u0026#34;author\u0026#34;: {\u0026#34;age\u0026#34;: 31}} In order to preserve objects and have their fields merged instead of overwritten, we need to write a custom function.\nHere is the full function which recursively merges two JSON objects A  and B:\nCREATE OR REPLACE FUNCTION jsonb_recursive_merge(A jsonb, B jsonb) RETURNS jsonb LANGUAGE SQL AS $$ SELECT jsonb_object_agg( coalesce(ka, kb), CASE WHEN va isnull THEN vb WHEN vb isnull THEN va WHEN jsonb_typeof(va) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; OR jsonb_typeof(vb) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; THEN vb ELSE jsonb_recursive_merge(va, vb) END ) FROM jsonb_each(A) temptable1(ka, va) FULL JOIN jsonb_each(B) temptable2(kb, vb) ON ka = kb $$; This function may be a bit hard to digest, so let\u0026rsquo;s break it down:\nSELECT jsonb_object_agg( ... ) FROM jsonb_each(A) temptableA(ka, va) FULL JOIN jsonb_each(B) temptableB(kb, vb) ON ka = kb jsonb_object_agg is a built-in postgresql function which aggregates a list of (key, value) pairs into a JSON object. This is what creates the final merged JSON result. Here we are applying jsonb_object_agg on the results of an in-memory temporary table that we are creating on the fly.\nTemporary tables jsonb_each() is a built-in postgresql function that iterates a JSON object returning (key, value) pairs. We call this function on both input JSON object A and B and then store the results in temporary tables temptableA and temptableB respectively.\ntemptableA(ka, va) is the definition of a temporary table with columns ka and va for the key and value results of jsonb_each(). This is where ka and va are first introduced. We do the exact same thing for JSON object B to get kb and vb.\nNext we do a FULL JOIN with the two temporary tables on the key column. This gives us one table that has all the (key, value) pairs from both JSON objects A and B. Below is an example of what the results of that table may look like:\n   ka va kb vb     likes 5 likes 10     comments 3   shares 1     impressions 65 impressions 130    Table 1: An example of a FULL JOIN with two temporary tables produced by jsonb_each()\nIt is this table from which we select the input to jsonb_object_agg(). As we iterate through the rows of this joined temporary table, we need to determine which key (ka or kb) and value (va or vb) we want to place in the resultant JSON object.\nSelecting the Key coalesce(ka, kb) coalesce is a built in postgresql function that returns the first non null value it is given. In this case it will choose ka if kb is null or kb if ka is null. Since we performed our FULL JOIN on columns ka = kb, we are guaranteed to have a non-null value for either ka or kb. When both ka and kb are non-null, they will be the same value.\nSelecting the Value CASE WHEN va isnull THEN vb WHEN vb isnull THEN va WHEN jsonb_typeof(va) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; OR jsonb_typeof(vb) \u0026lt;\u0026gt; \u0026#39;object\u0026#39; THEN vb ELSE jsonb_recursive_merge(va, vb) END To select the value, we have a switch statement. The first two cases chooses the non-null value when one of the values is null. The third case is when both va and vb are defined and not both JSON objects themselves. In this case we choose vb over va (remember we are merging B into A). The final case (else) handles the situation where va and vb are both JSON objects. In that situation we recursively call the jsonb_recursive_merge on va and vb.\nUsing the function One common use for this function is to upsert a row. In an upsert, when the row exists, you want to update it and when it doesn\u0026rsquo;t, you want to insert a new one. To do this, you would use an INSERT statement with the ON CONFLICT (col1,..., colN) DO UPDATE SET clause. The columns in the clause specify the columns of a unique index. Following the clause is a list of column_name = \u0026lt;expression\u0026gt; statements that decide just how each column is to be updated.\nBelow is an example of updating a table of tweet metrics:\nINSERT INTO tweets (id, metrics) VALUES (1, \u0026#39;{\u0026#34;likes\u0026#34;: 22, \u0026#34;comments\u0026#34;: 12}\u0026#39;) ON CONFLICT (id) DO UPDATE SET metrics = jsonb_recursive_merge(tweets.metrics, excluded.metrics); In the statement above, if a row with the same ID exists, it will call the jsonb_recursive_merge function on the current value, tweets.metrics, and the inserted value, excluded.metrics (the excluded table is the name of the special table representing rows proposed for insertion[2]).\nLimitations When we designed our jsonb_recursive_merge function we had to decide what \u0026ldquo;merge\u0026rdquo; meant to us. We decided that an overwrite of a value constitutes a \u0026ldquo;merge\u0026rdquo;. But what about values that are arrays? One could argue that merging two arrays [1, 2, 3] and [4, 5, 6] should result in [1,2,3,4,5,6]. It really all depends on the context of what you are trying to do.\nIf you want to also merge the values of arrays you can add an extra case statement that appends the values when both va and vb are arrays:\nWHEN jsonb_typeof(va) = \u0026#39;array\u0026#39; AND jsonb_typeof(vb) = \u0026#39;array\u0026#39; THEN va || vb However, be aware that this will apply to all arrays encountered in the JSON objects.\nAnd there you have it, a custom PostgreSQL function that merges two JSON objects, preserving and merging any nested objects. I\u0026rsquo;d like to thank and give credit to klin and his very helpful StackOverflow answer which brought me to a solution to this problem.\nFurther Reading [1] JSON Functions and Operators\n[2] PostgresSQL Insert Documentation\n","ref":"/posts/recursive_jsonb_merge/"},{"title":"About","date":"","description":"","body":"I am a backend software developer that has a passion for software architecture, scalability, development efficiency and clean code. I love learning and sharing what I have learned with others.\nAside from work, I love to play electric guitar, renovate my home and be out in nature.\n","ref":"/about/"}]